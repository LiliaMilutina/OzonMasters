{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import zipfile\n",
    "import ast\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse json\n",
    "\n",
    "def prepare(df):\n",
    "    data = pd.DataFrame(list(df['user']))\n",
    "    data['session_id'] = df['session_id']\n",
    "\n",
    "    sites_for_user_list = []\n",
    "    theme_events_list = []\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        s = pd.DataFrame.from_dict(list(data['sites_for_user'])[0])\n",
    "        s['session_id'] = data.iloc[i, :]['session_id']\n",
    "        sites_for_user_list.append(s)\n",
    "        \n",
    "        t = pd.DataFrame.from_dict(data['theme_events'].tolist()[0])\n",
    "        t['session_id'] = data.iloc[i, :]['session_id']\n",
    "        theme_events_list.append(t)\n",
    "    \n",
    "    sites_for_user = pd.concat(sites_for_user_list)\n",
    "    sites_for_user['site_hash'] = sites_for_user['site_hash'].astype('str')\n",
    "    sites_for_user['session_id'] = sites_for_user['session_id'].astype('str')    \n",
    "    \n",
    "    theme_events = pd.concat(theme_events_list)\n",
    "    theme_events['site_hash'] = theme_events['site_hash'].astype('str')\n",
    "    theme_events['session_id'] = theme_events['session_id'].astype('str')\n",
    "    \n",
    "    data_info = data.iloc[:, :-3]\n",
    "    data_info['session_id'] = data['session_id']\n",
    "    data_info['session_id'] = data_info['session_id'].astype('str')\n",
    "    \n",
    "    df_actions = df[['session_id', 'actions']]\n",
    "    df_actions['site_hash'] = df_actions['actions'].apply(lambda x: list(x.keys()))\n",
    "    df_values = df_actions['actions'].apply(lambda x: list(x.values())).explode()\n",
    "    df_actions.drop(['actions'], axis=1, inplace=True)\n",
    "\n",
    "    df_actions = df_actions.explode('site_hash')\n",
    "    \n",
    "    df_actions.reset_index(inplace=True)\n",
    "    df_actions.drop(columns='index', inplace=True)\n",
    "    \n",
    "    df_actions['site_hash'] = df_actions['site_hash'].astype('str')\n",
    "    df_actions['session_id'] = df_actions['session_id'].astype('str')\n",
    "    df_actions['values'] = df_values.values\n",
    "    df_actions['values'] = df_actions['values'].astype('int32')\n",
    "    \n",
    "    # merge all\n",
    "    \n",
    "    df_ready = df_actions.merge(sites_for_user, on = ['site_hash', 'session_id'], how='left')\n",
    "    df_ready = df_ready.merge(theme_events, on = ['site_hash', 'session_id'], how='left')\n",
    "    df_ready = df_ready.merge(data_info, on=['session_id'], how='left')\n",
    "    \n",
    "    df_ready['visits'] = df_ready['visits'].astype('float32', errors='ignore')\n",
    "    df_ready['uniq_urls'] = df_ready['uniq_urls'].astype('float32', errors='ignore')\n",
    "    df_ready['last_ts'] = df_ready['last_ts'].astype('float32', errors='ignore')\n",
    "    df_ready['clicks'] = df_ready['clicks'].astype('float32', errors='ignore')\n",
    "    df_ready['shows'] = df_ready['shows'].astype('float32', errors='ignore')\n",
    "    df_ready['last_click_ts'] = df_ready['last_click_ts'].astype('float32', errors='ignore')\n",
    "    df_ready['vid'] = df_ready['vid'].astype('category', errors='ignore')\n",
    "    df_ready['vk_id'] = df_ready['vk_id'].astype('category', errors='ignore')\n",
    "    df_ready['ok_id'] = df_ready['ok_id'].astype('category', errors='ignore')\n",
    "    df_ready['email'] = df_ready['email'].astype('category', errors='ignore')\n",
    "    df_ready['age'] = df_ready['age'].astype('int32', errors='ignore')\n",
    "    df_ready['gender'] = df_ready['gender'].astype('category', errors='ignore')\n",
    "    df_ready['geo'] = df_ready['geo'].astype('int32', errors='ignore')\n",
    "    \n",
    "    return df_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe(archive, name, k):\n",
    "    i = 0\n",
    "    df_list = []\n",
    "    df_test = pd.DataFrame()\n",
    "    with archive.open(name) as f:\n",
    "        for line in f.readlines():\n",
    "            d = json.loads(line)\n",
    "            df_el = pd.DataFrame.from_dict(d, orient='index', columns = [i]).T\n",
    "            df_list.append(df_el)\n",
    "            i += 1\n",
    "\n",
    "    df_test = pd.concat(df_list)\n",
    "    \n",
    "    df_ = prepare(df_test)\n",
    "    df_.to_csv('df_test'+ str(k), index=False)\n",
    "    print(k)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files from archive and make dataframe\n",
    "\n",
    "archive = ZipFile('test.zip', 'r')\n",
    "\n",
    "names = archive.namelist()\n",
    "\n",
    "data_test_list = Parallel(n_jobs=4)(delayed(make_dataframe(archive, names[k], k)) for k in range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all dataframes in one\n",
    "\n",
    "test_list = []\n",
    "for k in range(100):\n",
    "    d = pd.read_csv('df_test'+ str(k))\n",
    "    test_list.append(d)\n",
    "    if k % 10 == 0:\n",
    "        print(k)\n",
    "data_test = pd.concat(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add site_hash_types\n",
    "\n",
    "site_hash_types = pd.read_csv('site_hash_types.tsv', sep='\\t', header=None, names=['site_hash', 'type'])\n",
    "le = preprocessing.LabelEncoder()\n",
    "site_hash_types['type'] = le.fit_transform(site_hash_types['type'])\n",
    "\n",
    "data_test = data_test.merge(site_hash_types, on=['site_hash'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add site_hash_weights\n",
    "\n",
    "site_hash_weights = pd.read_csv('site_hash_weights.tsv', sep='\\t', header=None, names=['site_hash', 'weight'])\n",
    "site_hash_weights.drop_duplicates(['site_hash'], inplace=True)\n",
    "data_test = data_test.merge(site_hash_weights, on=['site_hash'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.to_csv('data_train', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
