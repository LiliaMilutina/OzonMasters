{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecSys2019_DeepLearning_Evaluation\n",
    "\n",
    "https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation\n",
    "\n",
    "Установка:\n",
    "pip install -r requirements.txt\n",
    "\n",
    "$ pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Collaborative Filtering\n",
    "https://dl.acm.org/doi/pdf/10.1145/3038912.3052569"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Base.BaseRecommender import BaseRecommender\n",
    "from Base.Incremental_Training_Early_Stopping import Incremental_Training_Early_Stopping\n",
    "from Base.DataIO import DataIO\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "import os\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.models import Model, load_model, save_model, clone_model\n",
    "from keras.layers import Embedding, Input, Dense, Reshape, Flatten, Dropout, Concatenate, Multiply\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_get_model(num_users, num_items, layers = [20,10], reg_layers=[0,0]):\n",
    "    assert len(layers) == len(reg_layers)\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = int(layers[0]/2), name = 'user_embedding',\n",
    "                                   embeddings_initializer = 'random_normal', embeddings_regularizer = l2(reg_layers[0]), input_length=1)\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = int(layers[0]/2), name = 'item_embedding',\n",
    "                                   embeddings_initializer = 'random_normal', embeddings_regularizer = l2(reg_layers[0]), input_length=1)\n",
    "\n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "\n",
    "    # The 0-th layer is the concatenation of embedding layers\n",
    "    vector = Concatenate()([user_latent, item_latent])\n",
    "\n",
    "    # MLP layers\n",
    "    for idx in range(1, num_layer):\n",
    "        layer = Dense(layers[idx], kernel_regularizer = l2(reg_layers[idx]), activation='relu', name = 'layer%d' %idx)\n",
    "        vector = layer(vector)\n",
    "\n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = 'prediction')(vector)\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input],\n",
    "                  outputs=prediction)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def GMF_get_model(num_users, num_items, latent_dim, regs=[0,0]):\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding',\n",
    "                                  embeddings_initializer = 'random_normal', embeddings_regularizer = l2(regs[0]), input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding',\n",
    "                                  embeddings_initializer = 'random_normal', embeddings_regularizer = l2(regs[1]), input_length=1)\n",
    "\n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "\n",
    "    # Element-wise product of user and item embeddings\n",
    "    predict_vector = Multiply()([user_latent, item_latent])\n",
    "\n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = 'prediction')(predict_vector)\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input],\n",
    "                outputs=prediction)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def NeuCF_get_model(num_users, num_items, mf_dim=10, layers=[10], reg_layers=[0], reg_mf=0.0):\n",
    "    assert len(layers) == len(reg_layers)\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    # Embedding layer\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = mf_dim, name = 'mf_embedding_user',\n",
    "                                  embeddings_initializer = 'random_normal', embeddings_regularizer = l2(reg_mf), input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = mf_dim, name = 'mf_embedding_item',\n",
    "                                  embeddings_initializer = 'random_normal', embeddings_regularizer = l2(reg_mf), input_length=1)\n",
    "\n",
    "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = int(layers[0]/2), name = \"mlp_embedding_user\",\n",
    "                                   embeddings_initializer = 'random_normal', embeddings_regularizer = l2(reg_layers[0]), input_length=1)\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = int(layers[0]/2), name = 'mlp_embedding_item',\n",
    "                                   embeddings_initializer = 'random_normal', embeddings_regularizer = l2(reg_layers[0]), input_length=1)\n",
    "\n",
    "    # MF part\n",
    "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    mf_vector = Multiply()([mf_user_latent, mf_item_latent]) # element-wise multiply\n",
    "\n",
    "    # MLP part\n",
    "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "    mlp_vector = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
    "    for idx in range(1, num_layer):\n",
    "        layer = Dense(layers[idx], kernel_regularizer= l2(reg_layers[idx]), activation='relu', name=\"layer%d\" %idx)\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    # Concatenate MF and MLP parts\n",
    "    predict_vector = Concatenate()([mf_vector, mlp_vector])\n",
    "\n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = \"prediction\")(predict_vector)\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input],\n",
    "                  outputs=prediction)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def load_pretrain_model(model, gmf_model, mlp_model, num_layers):\n",
    "    # MF embeddings\n",
    "    gmf_user_embeddings = gmf_model.get_layer('user_embedding').get_weights()\n",
    "    gmf_item_embeddings = gmf_model.get_layer('item_embedding').get_weights()\n",
    "    model.get_layer('mf_embedding_user').set_weights(gmf_user_embeddings)\n",
    "    model.get_layer('mf_embedding_item').set_weights(gmf_item_embeddings)\n",
    "\n",
    "    # MLP embeddings\n",
    "    mlp_user_embeddings = mlp_model.get_layer('user_embedding').get_weights()\n",
    "    mlp_item_embeddings = mlp_model.get_layer('item_embedding').get_weights()\n",
    "    model.get_layer('mlp_embedding_user').set_weights(mlp_user_embeddings)\n",
    "    model.get_layer('mlp_embedding_item').set_weights(mlp_item_embeddings)\n",
    "\n",
    "    # MLP layers\n",
    "    for i in range(1, num_layers):\n",
    "        mlp_layer_weights = mlp_model.get_layer('layer%d' %i).get_weights()\n",
    "        model.get_layer('layer%d' %i).set_weights(mlp_layer_weights)\n",
    "\n",
    "    # Prediction weights\n",
    "    gmf_prediction = gmf_model.get_layer('prediction').get_weights()\n",
    "    mlp_prediction = mlp_model.get_layer('prediction').get_weights()\n",
    "    new_weights = np.concatenate((gmf_prediction[0], mlp_prediction[0]), axis=0)\n",
    "    new_b = gmf_prediction[1] + mlp_prediction[1]\n",
    "    model.get_layer('prediction').set_weights([0.5*new_weights, 0.5*new_b])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_train_instances(train, num_negatives, num_items):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances\n",
    "        for t in range(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while (u, j) in train.keys():#train.has_key((u, j)):\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels\n",
    "\n",
    "\n",
    "\n",
    "def set_learner(model, learning_rate, learner):\n",
    "\n",
    "    if learner.lower() == \"adagrad\":\n",
    "        model.compile(optimizer=Adagrad(lr=learning_rate), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"rmsprop\":\n",
    "        model.compile(optimizer=RMSprop(lr=learning_rate), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"adam\":\n",
    "        model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy')\n",
    "    else:\n",
    "        model.compile(optimizer=SGD(lr=learning_rate), loss='binary_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def deep_clone_model(source_model):\n",
    "\n",
    "    destination_model = clone_model(source_model)\n",
    "    destination_model.set_weights(source_model.get_weights())\n",
    "\n",
    "    return destination_model\n",
    "\n",
    "\n",
    "\n",
    "class NeuMF_RecommenderWrapper(BaseRecommender, Incremental_Training_Early_Stopping):\n",
    "\n",
    "\n",
    "    RECOMMENDER_NAME = \"NeuMF_RecommenderWrapper\"\n",
    "\n",
    "    def __init__(self, URM_train):\n",
    "        super(NeuMF_RecommenderWrapper, self).__init__(URM_train)\n",
    "\n",
    "        self._train = sps.dok_matrix(self.URM_train)\n",
    "        self.n_users, self.n_items = self.URM_train.shape\n",
    "\n",
    "        self._item_indices = np.arange(0, self.n_items, dtype=np.int)\n",
    "        self._user_ones_vector = np.ones_like(self._item_indices)\n",
    "\n",
    "\n",
    "    def _compute_item_score(self, user_id_array, items_to_compute=None):\n",
    "\n",
    "        item_scores = - np.ones((len(user_id_array), self.n_items)) * np.inf\n",
    "\n",
    "        for user_index in range(len(user_id_array)):\n",
    "\n",
    "            user_id = user_id_array[user_index]\n",
    "\n",
    "            # The prediction requires a list of two arrays user_id, item_id of equal length\n",
    "            # To compute the recommendations for a single user, we must provide its index as many times as the\n",
    "            # number of items\n",
    "            item_score_user = self.model.predict([self._user_ones_vector*user_id, self._item_indices],\n",
    "                                                 batch_size=100, verbose=0)\n",
    "\n",
    "\n",
    "            if items_to_compute is not None:\n",
    "                item_scores[user_index, items_to_compute] = item_score_user.ravel()[items_to_compute]\n",
    "            else:\n",
    "                item_scores[user_index, :] = item_score_user.ravel()\n",
    "\n",
    "\n",
    "        return item_scores\n",
    "\n",
    "\n",
    "    def get_early_stopping_final_epochs_dict(self):\n",
    "        \"\"\"\n",
    "        This function returns a dictionary to be used as optimal parameters in the .fit() function\n",
    "        It provides the flexibility to deal with multiple early-stopping in a single algorithm\n",
    "        e.g. in NeuMF there are three model componets each with its own optimal number of epochs\n",
    "        the return dict would be {\"epochs\": epochs_best_neumf, \"epochs_gmf\": epochs_best_gmf, \"epochs_mlp\": epochs_best_mlp}\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        return {\"epochs\": self.epochs_best, \"epochs_gmf\": self.epochs_best_gmf, \"epochs_mlp\": self.epochs_best_mlp}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self,\n",
    "            epochs = 100,\n",
    "            epochs_gmf=100,\n",
    "            epochs_mlp=100,\n",
    "            batch_size = 256,\n",
    "            num_factors = 8,\n",
    "            layers = [64,32,16,8],\n",
    "            reg_mf = 0.0,\n",
    "            reg_layers = [0,0,0,0],\n",
    "            num_negatives = 4,\n",
    "            learning_rate = 1e-3,\n",
    "            learning_rate_pretrain = 1e-3,\n",
    "            learner = 'sgd',\n",
    "            learner_pretrain = 'adam',\n",
    "            pretrain = True,\n",
    "            root_folder_pretrain = None,\n",
    "            **earlystopping_kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param epochs:\n",
    "        :param batch_size:\n",
    "        :param num_factors: Embedding size of MF model\n",
    "        :param layers: MLP layers. Note that the first layer is the concatenation of user and item embeddings. So layers[0]/2 is the embedding size.\n",
    "        :param reg_mf: Regularization for MF embeddings.\n",
    "        :param reg_layers: Regularization for each MLP layer. reg_layers[0] is the regularization for embeddings.\n",
    "        :param num_negatives: Number of negative instances to pair with a positive instance.\n",
    "        :param learning_rate:\n",
    "        :param learning_rate_pretrain:\n",
    "        :param learner: adagrad, adam, rmsprop, sgd\n",
    "        :param learner_pretrain: adagrad, adam, rmsprop, sgd\n",
    "        :param root_folder_pretrain: Specify the pretrain model folder where to save MF and MLP for MF part.\n",
    "        :param do_pretrain:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.mf_dim = num_factors\n",
    "        self.layers = layers.copy()\n",
    "        self.reg_mf = reg_mf\n",
    "        self.reg_layers = reg_layers.copy()\n",
    "        self.num_negatives = num_negatives\n",
    "\n",
    "        assert learner in [\"adagrad\", \"adam\", \"rmsprop\", \"sgd\"]\n",
    "        assert learner_pretrain in [\"adagrad\", \"adam\", \"rmsprop\", \"sgd\"]\n",
    "\n",
    "        self.pretrain = pretrain\n",
    "\n",
    "        if self.pretrain:\n",
    "\n",
    "            if root_folder_pretrain is not None:\n",
    "                print(\"NeuMF_RecommenderWrapper: pretrained models will be saved in '{}'\".format(root_folder_pretrain))\n",
    "\n",
    "                # If directory does not exist, create\n",
    "                if not os.path.exists(root_folder_pretrain):\n",
    "                    os.makedirs(root_folder_pretrain)\n",
    "\n",
    "            print(\"NeuMF_RecommenderWrapper: root_folder_pretrain not provided, pretrained models will not be saved\")\n",
    "\n",
    "            print(\"NeuMF_RecommenderWrapper: Pretraining GMF...\")\n",
    "\n",
    "            self.model = GMF_get_model(self.n_users, self.n_items, self.mf_dim)\n",
    "            self.model = set_learner(self.model, learning_rate_pretrain, learner_pretrain)\n",
    "\n",
    "            self._best_model = deep_clone_model(self.model)\n",
    "\n",
    "            self._train_with_early_stopping(epochs_gmf,\n",
    "                                            algorithm_name = self.RECOMMENDER_NAME,\n",
    "                                            **earlystopping_kwargs)\n",
    "\n",
    "            self.epochs_best_gmf = self.epochs_best\n",
    "\n",
    "            if root_folder_pretrain is not None:\n",
    "                model_out_file = \"GMF_factors_{}_pretrain\".format(self.mf_dim)\n",
    "                self._best_model.save_weights(root_folder_pretrain + model_out_file, overwrite=True)\n",
    "\n",
    "            self.gmf_model = deep_clone_model(self._best_model)\n",
    "\n",
    "\n",
    "\n",
    "            print(\"NeuMF_RecommenderWrapper: Pretraining MLP...\")\n",
    "\n",
    "            self.model = MLP_get_model(self.n_users, self.n_items, self.layers, self.reg_layers)\n",
    "            self.model = set_learner(self.model, learning_rate_pretrain, learner_pretrain)\n",
    "\n",
    "            self._best_model = deep_clone_model(self.model)\n",
    "\n",
    "            self._train_with_early_stopping(epochs_mlp,\n",
    "                                            algorithm_name = self.RECOMMENDER_NAME,\n",
    "                                            **earlystopping_kwargs)\n",
    "\n",
    "            self.epochs_best_mlp = self.epochs_best\n",
    "\n",
    "            if root_folder_pretrain is not None:\n",
    "                model_out_file = \"MLP_layers_{}_reg_layers_{}_pretrain\".format(self.layers, reg_layers)\n",
    "                self._best_model.save_weights(root_folder_pretrain + model_out_file, overwrite=True)\n",
    "\n",
    "            self.mlp_model = deep_clone_model(self._best_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Build model\n",
    "        self.model = NeuCF_get_model(self.n_users, self.n_items, self.mf_dim, self.layers, self.reg_layers, self.reg_mf)\n",
    "        self.model = set_learner(self.model, learning_rate, learner)\n",
    "\n",
    "\n",
    "        # Load pretrain model\n",
    "        if pretrain:\n",
    "            self.model = load_pretrain_model(self.model, self.gmf_model, self.mlp_model, len(layers))\n",
    "            print(\"NeuMF_RecommenderWrapper: Load pretrained GMF and MLP models.\")\n",
    "\n",
    "\n",
    "        print(\"NeuMF_RecommenderWrapper: Training NeuCF...\")\n",
    "\n",
    "        self._best_model = deep_clone_model(self.model)\n",
    "\n",
    "        self._train_with_early_stopping(epochs,\n",
    "                                        algorithm_name = self.RECOMMENDER_NAME,\n",
    "                                        **earlystopping_kwargs)\n",
    "\n",
    "\n",
    "        print(\"NeuMF_RecommenderWrapper: Tranining complete\")\n",
    "\n",
    "        self.model = deep_clone_model(self._best_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _prepare_model_for_validation(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _update_best_model(self):\n",
    "        # Keras only clones the structure of the model, not the weights\n",
    "        self._best_model = deep_clone_model(self.model)\n",
    "\n",
    "\n",
    "    def _run_epoch(self, currentEpoch):\n",
    "\n",
    "        # Generate training instances\n",
    "        user_input, item_input, labels = get_train_instances(self._train, self.num_negatives, self.n_items)\n",
    "\n",
    "        # Training\n",
    "        hist = self.model.fit([np.array(user_input), np.array(item_input)], #input\n",
    "                         np.array(labels), # labels\n",
    "                         batch_size=self.batch_size, epochs=1, verbose=0, shuffle=True)\n",
    "\n",
    "        print(\"NeuMF_RecommenderWrapper: Epoch {}, loss {:.2E}\".format(currentEpoch+1, hist.history['loss'][0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save_model(self, folder_path, file_name = None):\n",
    "\n",
    "        if file_name is None:\n",
    "            file_name = self.RECOMMENDER_NAME\n",
    "\n",
    "        self._print(\"Saving model in file '{}'\".format(folder_path + file_name))\n",
    "\n",
    "        self.model.save_weights(folder_path + file_name + \"_weights\", overwrite=True)\n",
    "\n",
    "        data_dict_to_save = {\n",
    "            \"n_users\": self.n_users,\n",
    "            \"n_items\": self.n_items,\n",
    "            \"mf_dim\": self.mf_dim,\n",
    "            \"layers\": self.layers,\n",
    "            \"reg_layers\": self.reg_layers,\n",
    "            \"reg_mf\": self.reg_mf,\n",
    "        }\n",
    "\n",
    "        dataIO = DataIO(folder_path=folder_path)\n",
    "        dataIO.save_data(file_name=file_name, data_dict_to_save = data_dict_to_save)\n",
    "\n",
    "\n",
    "        self._print(\"Saving complete\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def load_model(self, folder_path, file_name = None):\n",
    "\n",
    "        if file_name is None:\n",
    "            file_name = self.RECOMMENDER_NAME\n",
    "\n",
    "        self._print(\"Loading model from file '{}'\".format(folder_path + file_name))\n",
    "\n",
    "        dataIO = DataIO(folder_path=folder_path)\n",
    "        data_dict = dataIO.load_data(file_name=file_name)\n",
    "\n",
    "        for attrib_name in data_dict.keys():\n",
    "             self.__setattr__(attrib_name, data_dict[attrib_name])\n",
    "\n",
    "\n",
    "        self.model = NeuCF_get_model(self.n_users, self.n_items, self.mf_dim, self.layers, self.reg_layers, self.reg_mf)\n",
    "        self.model.load_weights(folder_path + file_name + \"_weights\")\n",
    "\n",
    "\n",
    "        self._print(\"Loading complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run_WWW_17_NeuMF.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 22/11/17\n",
    "\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "from Recommender_import_list import *\n",
    "from Conferences.WWW.NeuMF_our_interface.NeuMF_RecommenderWrapper import NeuMF_RecommenderWrapper\n",
    "\n",
    "\n",
    "from ParameterTuning.run_parameter_search import runParameterSearch_Collaborative\n",
    "from ParameterTuning.SearchSingleCase import SearchSingleCase\n",
    "from ParameterTuning.SearchAbstractClass import SearchInputRecommenderArgs\n",
    "\n",
    "import os, traceback, argparse\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "from Utils.assertions_on_data_for_experiments import assert_implicit_data, assert_disjoint_matrices\n",
    "from Utils.plot_popularity import plot_popularity_bias, save_popularity_statistics\n",
    "\n",
    "\n",
    "from Utils.ResultFolderLoader import ResultFolderLoader, generate_latex_hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "def read_data_split_and_search(dataset_name,\n",
    "                                   flag_baselines_tune = False,\n",
    "                                   flag_DL_article_default = False, flag_DL_tune = False,\n",
    "                                   flag_print_results = False):\n",
    "\n",
    "\n",
    "    from Conferences.WWW.NeuMF_our_interface.Movielens1M.Movielens1MReader import Movielens1MReader\n",
    "    from Conferences.WWW.NeuMF_our_interface.Pinterest.PinterestICCVReader import PinterestICCVReader\n",
    "\n",
    "\n",
    "    result_folder_path = \"result_experiments/{}/{}_{}/\".format(CONFERENCE_NAME, ALGORITHM_NAME, dataset_name)\n",
    "\n",
    "\n",
    "    if dataset_name == \"movielens1m\":\n",
    "        dataset = Movielens1MReader(result_folder_path)\n",
    "\n",
    "    elif dataset_name == \"pinterest\":\n",
    "        dataset = PinterestICCVReader(result_folder_path)\n",
    "\n",
    "\n",
    "    URM_train = dataset.URM_DICT[\"URM_train\"].copy()\n",
    "    URM_validation = dataset.URM_DICT[\"URM_validation\"].copy()\n",
    "    URM_test = dataset.URM_DICT[\"URM_test\"].copy()\n",
    "    URM_test_negative = dataset.URM_DICT[\"URM_test_negative\"].copy()\n",
    "\n",
    "\n",
    "    # Ensure IMPLICIT data and DISJOINT sets\n",
    "    assert_implicit_data([URM_train, URM_validation, URM_test, URM_test_negative])\n",
    "\n",
    "    assert_disjoint_matrices([URM_train, URM_validation, URM_test])\n",
    "    assert_disjoint_matrices([URM_train, URM_validation, URM_test_negative])\n",
    "\n",
    "\n",
    "\n",
    "    # If directory does not exist, create\n",
    "    if not os.path.exists(result_folder_path):\n",
    "        os.makedirs(result_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "    algorithm_dataset_string = \"{}_{}_\".format(ALGORITHM_NAME, dataset_name)\n",
    "\n",
    "    plot_popularity_bias([URM_train + URM_validation, URM_test],\n",
    "                         [\"URM train\", \"URM test\"],\n",
    "                         result_folder_path + algorithm_dataset_string + \"popularity_plot\")\n",
    "\n",
    "    save_popularity_statistics([URM_train + URM_validation, URM_test],\n",
    "                               [\"URM train\", \"URM test\"],\n",
    "                               result_folder_path + algorithm_dataset_string + \"popularity_statistics\")\n",
    "\n",
    "\n",
    "\n",
    "    collaborative_algorithm_list = [\n",
    "        Random,\n",
    "        TopPop,\n",
    "        UserKNNCFRecommender,\n",
    "        ItemKNNCFRecommender,\n",
    "        P3alphaRecommender,\n",
    "        RP3betaRecommender,\n",
    "        PureSVDRecommender,\n",
    "        NMFRecommender,\n",
    "        IALSRecommender,\n",
    "        MatrixFactorization_BPR_Cython,\n",
    "        MatrixFactorization_FunkSVD_Cython,\n",
    "        EASE_R_Recommender,\n",
    "        SLIM_BPR_Cython,\n",
    "        SLIMElasticNetRecommender,\n",
    "        ]\n",
    "\n",
    "    metric_to_optimize = \"HIT_RATE\"\n",
    "    n_cases = 50\n",
    "    n_random_starts = 15\n",
    "\n",
    "\n",
    "    from Base.Evaluation.Evaluator import EvaluatorNegativeItemSample\n",
    "\n",
    "    evaluator_validation = EvaluatorNegativeItemSample(URM_validation, URM_test_negative, cutoff_list=[10])\n",
    "    evaluator_test = EvaluatorNegativeItemSample(URM_test, URM_test_negative, cutoff_list=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "\n",
    "    runParameterSearch_Collaborative_partial = partial(runParameterSearch_Collaborative,\n",
    "                                                       URM_train = URM_train,\n",
    "                                                       URM_train_last_test = URM_train + URM_validation,\n",
    "                                                       metric_to_optimize = metric_to_optimize,\n",
    "                                                       evaluator_validation_earlystopping = evaluator_validation,\n",
    "                                                       evaluator_validation = evaluator_validation,\n",
    "                                                       evaluator_test = evaluator_test,\n",
    "                                                       output_folder_path = result_folder_path,\n",
    "                                                       parallelizeKNN = False,\n",
    "                                                       allow_weighting = True,\n",
    "                                                       resume_from_saved = True,\n",
    "                                                       n_cases = n_cases,\n",
    "                                                       n_random_starts = n_random_starts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if flag_baselines_tune:\n",
    "\n",
    "        for recommender_class in collaborative_algorithm_list:\n",
    "            try:\n",
    "                runParameterSearch_Collaborative_partial(recommender_class)\n",
    "            except Exception as e:\n",
    "                print(\"On recommender {} Exception {}\".format(recommender_class, str(e)))\n",
    "                traceback.print_exc()\n",
    "\n",
    "\n",
    "\n",
    "    ################################################################################################\n",
    "    ######\n",
    "    ######      DL ALGORITHM\n",
    "    ######\n",
    "\n",
    "    if flag_DL_article_default:\n",
    "\n",
    "        try:\n",
    "\n",
    "\n",
    "            if dataset_name == \"movielens1m\":\n",
    "                num_factors = 64\n",
    "            elif dataset_name == \"pinterest\":\n",
    "                num_factors = 16\n",
    "\n",
    "\n",
    "            neuMF_article_hyperparameters = {\n",
    "                \"epochs\": 100,\n",
    "                \"epochs_gmf\": 100,\n",
    "                \"epochs_mlp\": 100,\n",
    "                \"batch_size\": 256,\n",
    "                \"num_factors\": num_factors,\n",
    "                \"layers\": [num_factors*4, num_factors*2, num_factors],\n",
    "                \"reg_mf\": 0.0,\n",
    "                \"reg_layers\": [0,0,0],\n",
    "                \"num_negatives\": 4,\n",
    "                \"learning_rate\": 1e-3,\n",
    "                \"learning_rate_pretrain\": 1e-3,\n",
    "                \"learner\": \"sgd\",\n",
    "                \"learner_pretrain\": \"adam\",\n",
    "                \"pretrain\": True\n",
    "            }\n",
    "\n",
    "\n",
    "            neuMF_earlystopping_hyperparameters = {\n",
    "                \"validation_every_n\": 5,\n",
    "                \"stop_on_validation\": True,\n",
    "                \"evaluator_object\": evaluator_validation,\n",
    "                \"lower_validations_allowed\": 5,\n",
    "                \"validation_metric\": metric_to_optimize\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "            parameterSearch = SearchSingleCase(NeuMF_RecommenderWrapper,\n",
    "                                               evaluator_validation=evaluator_validation,\n",
    "                                               evaluator_test=evaluator_test)\n",
    "\n",
    "            recommender_input_args = SearchInputRecommenderArgs(\n",
    "                                                CONSTRUCTOR_POSITIONAL_ARGS = [URM_train],\n",
    "                                                FIT_KEYWORD_ARGS = neuMF_earlystopping_hyperparameters)\n",
    "\n",
    "            recommender_input_args_last_test = recommender_input_args.copy()\n",
    "            recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS[0] = URM_train + URM_validation\n",
    "\n",
    "            parameterSearch.search(recommender_input_args,\n",
    "                                   recommender_input_args_last_test = recommender_input_args_last_test,\n",
    "                                   fit_hyperparameters_values=neuMF_article_hyperparameters,\n",
    "                                   output_folder_path = result_folder_path,\n",
    "                                   resume_from_saved = True,\n",
    "                                   output_file_name_root = NeuMF_RecommenderWrapper.RECOMMENDER_NAME)\n",
    "\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            print(\"On recommender {} Exception {}\".format(NeuMF_RecommenderWrapper, str(e)))\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ################################################################################################\n",
    "    ######\n",
    "    ######      PRINT RESULTS\n",
    "    ######\n",
    "\n",
    "    if flag_print_results:\n",
    "\n",
    "        n_test_users = np.sum(np.ediff1d(URM_test.indptr)>=1)\n",
    "        file_name = \"{}..//{}_{}_\".format(result_folder_path, ALGORITHM_NAME, dataset_name)\n",
    "\n",
    "        result_loader = ResultFolderLoader(result_folder_path,\n",
    "                                         base_algorithm_list = None,\n",
    "                                         other_algorithm_list = [NeuMF_RecommenderWrapper],\n",
    "                                         KNN_similarity_list = KNN_similarity_to_report_list,\n",
    "                                         ICM_names_list = None,\n",
    "                                         UCM_names_list = None)\n",
    "\n",
    "\n",
    "        result_loader.generate_latex_results(file_name + \"{}_latex_results.txt\".format(\"article_metrics\"),\n",
    "                                           metrics_list = [\"HIT_RATE\", \"NDCG\"],\n",
    "                                           cutoffs_list = [1, 5, 10],\n",
    "                                           table_title = None,\n",
    "                                           highlight_best = True)\n",
    "\n",
    "        result_loader.generate_latex_results(file_name + \"{}_latex_results.txt\".format(\"all_metrics\"),\n",
    "                                           metrics_list = [\"PRECISION\", \"RECALL\", \"MAP\", \"MRR\", \"NDCG\", \"F1\", \"HIT_RATE\", \"ARHR\",\n",
    "                                                           \"NOVELTY\", \"DIVERSITY_MEAN_INTER_LIST\", \"DIVERSITY_HERFINDAHL\", \"COVERAGE_ITEM\", \"DIVERSITY_GINI\", \"SHANNON_ENTROPY\"],\n",
    "                                           cutoffs_list = [10],\n",
    "                                           table_title = None,\n",
    "                                           highlight_best = True)\n",
    "\n",
    "        result_loader.generate_latex_time_statistics(file_name + \"{}_latex_results.txt\".format(\"time\"),\n",
    "                                           n_evaluation_users=n_test_users,\n",
    "                                           table_title = None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ALGORITHM_NAME = \"NeuMF\"\n",
    "    CONFERENCE_NAME = \"WWW\"\n",
    "\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-b', '--baseline_tune',        help=\"Baseline hyperparameter search\", type = bool, default = False)\n",
    "    parser.add_argument('-a', '--DL_article_default',   help=\"Train the DL model with article hyperparameters\", type = bool, default = False)\n",
    "    parser.add_argument('-p', '--print_results',        help=\"Print results\", type = bool, default = True)\n",
    "\n",
    "    input_flags = parser.parse_args()\n",
    "    print(input_flags)\n",
    "\n",
    "    KNN_similarity_to_report_list = [\"cosine\", \"dice\", \"jaccard\", \"asymmetric\", \"tversky\"]\n",
    "\n",
    "\n",
    "\n",
    "    dataset_list = [\"movielens1m\", \"pinterest\"]\n",
    "    dataset_list = [\"movielens1m\"] #отредактировано\n",
    "\n",
    "    for dataset_name in dataset_list:\n",
    "\n",
    "        read_data_split_and_search(dataset_name,\n",
    "                                        flag_baselines_tune=input_flags.baseline_tune,\n",
    "                                        flag_DL_article_default= input_flags.DL_article_default,\n",
    "                                        flag_print_results = input_flags.print_results,\n",
    "                                        )\n",
    "\n",
    "\n",
    "\n",
    "    if input_flags.print_results:\n",
    "        generate_latex_hyperparameters(result_folder_path =\"result_experiments/{}/\".format(CONFERENCE_NAME),\n",
    "                                       algorithm_name= ALGORITHM_NAME,\n",
    "                                       experiment_subfolder_list = dataset_list,\n",
    "                                       other_algorithm_list = [NeuMF_RecommenderWrapper],\n",
    "                                       KNN_similarity_to_report_list = KNN_similarity_to_report_list,\n",
    "                                       split_per_algorithm_type = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание:\n",
    "1. Запустить run_WWW_17_NeuMF.py -b True и выяснить какой из алгоритмов collaborative_algorithm_list оказался лучше\n",
    "для датасэта movielens1m\n",
    "2. Сравните num_factors=32 и num_factors=64\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
