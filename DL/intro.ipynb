{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "  dl1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQwlunyE86kE"
      },
      "source": [
        "# 1. Практическое задание. Обучение полносвязной нейронной сети.\n",
        "\n",
        "**ФИО**: Милютина Лилия Александровна\n",
        "\n",
        "**Дедлайн**: 3 октября 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyCGfNgA86kF"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "\n",
        "from glob import glob\n",
        "from collections import OrderedDict\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch.autograd import Function\n",
        "from torch.autograd import gradcheck\n",
        "from torch.optim import Optimizer, Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFaWrLT4GkfY"
      },
      "source": [
        "## 1. Загрузка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R930jTYMG6nB"
      },
      "source": [
        "Если вам требуется работать с каким-нибубь датасетом, то во-первых, проверьте нет ли его среди датасетов `torchvison`. \n",
        "\n",
        "Мы будем работать с `FashionMNIST`. Скачаем его, но будем использовать это только как быстрый и удобный способ скачать данные. \n",
        "\n",
        "Структруру класса датасет реализуем сами."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LauGpMvGF5qr",
        "outputId": "5a8a1b22-35ab-42c3-b559-a1b6029d7853"
      },
      "source": [
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIT32OCMaLLP"
      },
      "source": [
        "Воспользуемся функций загрузки данных из репозитория датасета."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm_cf_hEIapm",
        "outputId": "ea025cfb-b10d-4110-b417-8d76ccd45152"
      },
      "source": [
        "! ls data/FashionMNIST/raw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t10k-images-idx3-ubyte\t   train-images-idx3-ubyte\n",
            "t10k-images-idx3-ubyte.gz  train-images-idx3-ubyte.gz\n",
            "t10k-labels-idx1-ubyte\t   train-labels-idx1-ubyte\n",
            "t10k-labels-idx1-ubyte.gz  train-labels-idx1-ubyte.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xnY5xk1KBg2"
      },
      "source": [
        "#https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py\n",
        "\n",
        "def load_mnist(path, kind='train'):\n",
        "    import os\n",
        "    import gzip\n",
        "    import numpy as np\n",
        "\n",
        "    \"\"\"Load MNIST data from `path`\"\"\"\n",
        "    labels_path = os.path.join(path,\n",
        "                               '%s-labels-idx1-ubyte.gz'\n",
        "                               % kind)\n",
        "    images_path = os.path.join(path,\n",
        "                               '%s-images-idx3-ubyte.gz'\n",
        "                               % kind)\n",
        "\n",
        "    with gzip.open(labels_path, 'rb') as lbpath:\n",
        "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "                               offset=8)\n",
        "\n",
        "    with gzip.open(images_path, 'rb') as imgpath:\n",
        "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "                               offset=16).reshape(len(labels), 784)\n",
        "\n",
        "    return images, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHIlWRV0aZnA"
      },
      "source": [
        "Для удобства в PyTorch предоставляется ряд утилит для загрузки датасетов, их предварительной обработки и взаимодействия с ними. Эти вспомогательные классы находятся в модуле `torch.utils.data`. Здесь следует обратить внимание на две основные концепции:\n",
        "\n",
        "- `Dataset`, инкапсулирующий источник данных,\n",
        "- `DataLoader`, отвечающий за загрузку датасета, возможно, в параллельном режиме.\n",
        "\n",
        "Для создания новых датасетов наследуется класс `torch.utils.data.Dataset` и переопределяется метод `__len__`, так, чтобы он возвращал количество образцов в датасете, а также метод `__getitem__` для доступа к единичному значению по конкретному индексу. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jf2e5cPbJV2"
      },
      "source": [
        "Реализуем класс для FasionMnist.\n",
        "\n",
        "Элементами датасета должны являться пары `(np.array, np.array)`, массивы имеют следующие размерности и типы: `(28, 28)`, тип элемента `np.float32`; и `(1,)`, тип элемента `int`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snTBHRTQI1bc"
      },
      "source": [
        "class FashionMnist(Dataset):\n",
        "    def __init__(self, path, train=True, image_transform=None, \n",
        "                 label_transform=None):\n",
        "        if train:\n",
        "            images, labels = load_mnist(f\"{path}/raw\")\n",
        "        else:\n",
        "            images, labels = load_mnist(f\"{path}/raw\", kind=\"t10k\")\n",
        "        \n",
        "        self.img_labels = labels\n",
        "        self.transform = image_transform\n",
        "        self.target_transform = label_transform\n",
        "\n",
        "        image_size = int(np.sqrt(images.shape[1]))\n",
        "        self.images = images.reshape(-1, image_size, image_size)\n",
        "\n",
        "    def __len__(self,):\n",
        "        length = len(self.img_labels)\n",
        "        return length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = np.asarray(self.img_labels[idx])\n",
        "        img = self.images[idx, :, :]\n",
        "        if self.transform:\n",
        "          img = self.transform(img)\n",
        "        if self.target_transform:\n",
        "          label = self.target_transform(label)\n",
        "        return img, label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVGv_2zBNfpz"
      },
      "source": [
        "test_dataset = FashionMnist(\"data/FashionMNIST\", train=False)\n",
        "train_dataset = FashionMnist(\"data/FashionMNIST\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JciETIfndiGR"
      },
      "source": [
        "Визуализируйте случайные элементы датасета."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "VotUn-XNrq24",
        "outputId": "0835c3e1-ae12-47ac-c7f5-9f733a76649c"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "  random_image_ind = np.random.randint(0, len(test_dataset))\n",
        "  image, label = test_dataset[random_image_ind]\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(image, cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 25 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "320RvzL7PZrI"
      },
      "source": [
        "В конструктор датасета обычно можно передать преобразование `torchvision.transforms`. С помощью которых можно преобразовать исходные данные. \n",
        "\n",
        "Такое преобразование может привести, например, исходные данные к тензорам.\n",
        "\n",
        "Реализуйте поддержку преобразований в вашем датасете. Проверьте, что приведение типов работает корректно. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQsGbfXUTxcx"
      },
      "source": [
        "class ToTensor:\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        return torch.from_numpy(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BpFQ_Y2PoeI"
      },
      "source": [
        "transform = ToTensor()\n",
        "\n",
        "test_dataset = FashionMnist(\"data/FashionMNIST\", \n",
        "                            train=False, \n",
        "                            image_transform=transform, \n",
        "                            label_transform=transform\n",
        "                            )\n",
        "train_dataset = FashionMnist(\"data/FashionMNIST\",\n",
        "                             image_transform=transform, \n",
        "                             label_transform=transform\n",
        "                             )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukLn9rY7SCsS",
        "outputId": "066f5bfe-baef-41ca-de6c-d190f2b042e4"
      },
      "source": [
        "print(f\"The type of the data is {type(test_dataset[0][0])}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The type of the data is <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ltavx0QUMnm"
      },
      "source": [
        "Элементы датасета могут быть объединены в батчи явно и неявно. Если данные могут быть сконкатенированы или объединены каким-нибудь тривиальным способом, то можно не передавать никаких дополнительных парамертов в `torch.utils.data.Dataloader`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cizxx6m0VAI7"
      },
      "source": [
        "test_dataloader = DataLoader(test_dataset, batch_size=15, num_workers=2, shuffle=True)\n",
        "batch = next(iter(test_dataloader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIZiPrBgVwGA",
        "outputId": "de50c57d-cfe8-42cc-a2d9-830b1aa5076f"
      },
      "source": [
        "print(f\"The length of the batch is {len(batch)}\")\n",
        "print(f\"The shape of the batch[0] is {batch[0].shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The length of the batch is 2\n",
            "The shape of the batch[0] is torch.Size([15, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCggRyOQWVIx"
      },
      "source": [
        "Однако, если наша структура данных не позволяет нам использовать дефолтное объединение в батч, то можно написать собственную функцию. \n",
        "\n",
        "Реализуйте функцию, преобразующую последовательность элементов массива в батч. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQVh93fmWSjA"
      },
      "source": [
        "def collate(batch):\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    for i, item in enumerate(batch):\n",
        "        img, label = item\n",
        "        imgs.append(img)\n",
        "        labels.append(label)\n",
        "    imgs = torch.nn.utils.rnn.pad_sequence(imgs, batch_first=True, padding_value=0)\n",
        "    return imgs, torch.tensor(labels).type(torch.LongTensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wij6F-vpe7KS"
      },
      "source": [
        "Убедитесть, что все работает корректно. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dDtlvlCXNbW"
      },
      "source": [
        "test_dataloader = DataLoader(test_dataset, batch_size=15, num_workers=2, \n",
        "                             shuffle=True, collate_fn=collate)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=15, num_workers=2,\n",
        "                              shuffle=True, collate_fn=collate)\n",
        "batch = next(iter(test_dataloader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFN00IyZXYaF",
        "outputId": "0d8ff99b-7e1e-42c0-ee1d-04cd1f5ffc42"
      },
      "source": [
        "print(f\"The length of the batch is {len(batch)}\")\n",
        "print(f\"The shape of the batch[0] is {batch[0].shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The length of the batch is 2\n",
            "The shape of the batch[0] is torch.Size([15, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHcumwuF86kL"
      },
      "source": [
        "## 2. Реализация модулей нейронной сети (15 баллов)\n",
        "\n",
        "В этом разделе мы полностью реализуем модули для полносвязной сети. \n",
        "\n",
        "Для начала нам понадобится реализовать прямой и обратный проход через слои. \n",
        "\n",
        "Наши слои будут соответствовать следующему интерфейсу (на примере \"тождественного\" слоя):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzhCW5HcfmKd"
      },
      "source": [
        "Сначала, мы реализуем функцию и её градиенинт. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcqeFXxsFGQO"
      },
      "source": [
        "class IdentityFunction(Function):\n",
        "    \"\"\"\n",
        "    We can implement our own custom autograd Functions by subclassing\n",
        "    torch.autograd.Function and implementing the forward and backward passes\n",
        "    which operate on Tensors.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(x):\n",
        "        \"\"\"\n",
        "        In the forward pass we receive a Tensor containing the input and return\n",
        "        a Tensor containing the output. ctx is a context object that can be used\n",
        "        to stash information for backward computation. You can cache arbitrary\n",
        "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
        "        with respect to the output, and we need to compute the gradient of the loss\n",
        "        with respect to the input.\n",
        "        \"\"\"\n",
        "        return grad_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc6Rtczffxv4"
      },
      "source": [
        "Затем обернем функцию в `nn.Module()`, сделав её таким образом \"кирпичеком\" нашей \n",
        "\n",
        "---\n",
        "\n",
        "нейронки. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiqeRVcM86kM"
      },
      "source": [
        "class IdentityLayer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        # An identity layer does nothing\n",
        "        super().__init__()\n",
        "        self.identity = IdentityFunction.apply\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # An identity layer just returns whatever it gets as input.\n",
        "        return self.identity(inp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4IoM_pX86kQ"
      },
      "source": [
        "\n",
        "### 2.1 Слой нелинейности ReLU\n",
        "Для начала реализуем слой нелинейности $ReLU(x) = max(x, 0)$. Параметров у слоя нет. Метод forward должен вернуть результат поэлементного применения ReLU к входному массиву, метод backward - градиент функции потерь по входу слоя. В нуле будем считать производную равной 0. Обратите внимание, что при обратном проходе могут понадобиться величины, посчитанные во время прямого прохода, поэтому их стоит сохранить в `ctx`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9qrJ47xY6H0"
      },
      "source": [
        "class MyReLU(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad_input[input < 0] = 0\n",
        "        return grad_input\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y09ZVCsT86kT"
      },
      "source": [
        "class ReLU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    \n",
        "    def forward(self, input):\n",
        "        y = MyReLU.apply(input)\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RwKEUp_V3-L"
      },
      "source": [
        "Не забываем после реализации функции проверить градиент, испльзуя функцию `gradcheck`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0RnDQZCXXZn"
      },
      "source": [
        "relu = ReLU()\n",
        "x = torch.rand([16, 10], requires_grad=True).double()\n",
        "assert gradcheck(relu, x, eps=1e-4, atol=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qscLZQJKv21w"
      },
      "source": [
        "Сравнение с `PyTorch`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUmbmR4iXurd"
      },
      "source": [
        "our_relu = ReLU()\n",
        "torch_relu = torch.nn.ReLU()\n",
        "x = torch.rand([16, 10], requires_grad=True).double()\n",
        "\n",
        "assert torch.norm(torch_relu(x.float()) - our_relu(x)) < 1e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojTR4GFd86kY"
      },
      "source": [
        "### 2.2 Линейный слой\n",
        "Далее реализуем полносвязный слой без нелинейности. У слоя два параметра: матрица весов и вектор сдвига."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBl34oykbcBf"
      },
      "source": [
        "class Linear(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, inp, weight, bias):\n",
        "        ctx.save_for_backward(inp, weight, bias)\n",
        "        inp = inp.float()\n",
        "        output = inp.matmul(weight.t()) + bias\n",
        "        return output\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, weight, bias = ctx.saved_tensors\n",
        "        input = input.float()\n",
        "        grad_input = grad_weight = grad_bias = None\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = grad_output.matmul(weight)\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            grad_weight = grad_output.t().matmul(input)\n",
        "        if ctx.needs_input_grad[2]:\n",
        "            grad_bias = grad_output.sum(0)\n",
        "        return grad_input, grad_weight, grad_bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbN5JOc886kZ"
      },
      "source": [
        "class Dense(nn.Module):\n",
        "    def __init__(self, input_units, output_units):\n",
        "        super().__init__()\n",
        "        # initialize weights with small random numbers from normal distribution\n",
        "        # do not forget using nn.Parameter for bias and weight\n",
        "        self.input_units = input_units\n",
        "        self.output_units = output_units\n",
        "\n",
        "        self.weight = nn.Parameter(torch.normal(0, 0.01, size = (output_units, input_units)))\n",
        "        self.bias = nn.Parameter(torch.zeros(output_units))\n",
        "        \n",
        "    def forward(self,inp):\n",
        "        return Linear.apply(inp, self.weight, self.bias)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gToXI43WYMDv"
      },
      "source": [
        "Проверим градиент, а также сравним работу нашего модуля с имплементированным в `PyTorch`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et7mKX5xZfMI"
      },
      "source": [
        "Проверка градиента:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxLVozqLZb_H",
        "outputId": "918b3361-2c3b-40fe-da6e-23aa11e46f79"
      },
      "source": [
        "linear = Dense(10, 10)\n",
        "x = torch.rand([15, 10], requires_grad=True)\n",
        "assert gradcheck(linear, x, eps=1e-3, atol=1e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/autograd/gradcheck.py:633: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. \n",
            "  f'Input #{idx} requires gradient and '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW8ppOciZndN"
      },
      "source": [
        "Сравнение с `PyTorch`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIR9svs7Zmq0"
      },
      "source": [
        "weight = torch.normal(0, 1, (10, 10))\n",
        "bias = torch.zeros(10)\n",
        "x = torch.rand([10, 10], requires_grad=True)\n",
        "\n",
        "our_linear = Dense(10, 10)\n",
        "torch_linear = torch.nn.Linear(10, 10)\n",
        "\n",
        "state_dict = OrderedDict([(\"weight\", weight), (\"bias\", bias)])\n",
        "torch_linear.load_state_dict(state_dict)\n",
        "our_linear.load_state_dict(state_dict)\n",
        "\n",
        "assert torch.norm(torch_linear(x.float()) - our_linear(x)) < 1e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dyhDg0D86kt"
      },
      "source": [
        "### 2.3 LogSoftmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArL0HLGH86ku"
      },
      "source": [
        "Реализация softmax-слоя и функции потерь\n",
        "Для решения задачи многоклассовой классификации обычно используют softmax в качестве нелинейности на последнем слое, чтобы получить вероятности классов для каждого объекта:$$\\hat y = softmax(x)  = \\bigl \\{\\frac {exp(x_i)}{\\sum_j exp(x_j)} \\bigr \\}_{i=1}^K, \\quad K - \\text{число классов}$$В этом случае удобно оптимизировать логарифм правдоподобия:$$L(y, \\hat y) = -\\sum_{i=1}^K y_i \\log \\hat y_i \\rightarrow \\min,$$где $y_i=1$, если объект принадлежит $i$-му классу, и 0 иначе. Записанная в таком виде, эта функция потерь совпадает с выражением для кросс-энтропии. Очевидно, что ее также можно переписать через индексацию, если через $y_i$ обозначить класс данного объекта:$$L(y, \\hat y) = - \\log \\hat y_{y_i} \\rightarrow \\min$$В таком виде ее удобно реализовывать.\n",
        "\n",
        "Реализуйте слой LogSoftmax (без параметров). Метод forward должен вычислять логарифм от softmax, а метод backward - пропускать градиенты. В общем случае в промежуточных вычислениях backward получится трехмерный тензор, однако для нашей конкретной функции потерь все вычисления можно реализовать в матричном виде. Поэтому мы будем предполагать, что аргумент grad_output - это матрица, у которой в каждой строке только одно ненулевое значение (не обязательно единица)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSV3XD0N86ky"
      },
      "source": [
        "class MyLogSoftmax(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, inp):\n",
        "        result = torch.nn.functional.log_softmax(inp)\n",
        "        ctx.save_for_backward(inp)\n",
        "        return result\n",
        "    \n",
        "    @staticmethod        \n",
        "    def backward(ctx, grad_output):\n",
        "        input, = ctx.saved_tensors\n",
        "        result = torch.eye(input.shape[0]) - torch.nn.functional.softmax(input)\n",
        "        return grad_output.matmul(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7OnC6o_z2vg"
      },
      "source": [
        "class LogSoftmax(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return MyLogSoftmax.apply(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke4G67cddjVA"
      },
      "source": [
        "Проверка градиентов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfgkVgGbrQEv",
        "outputId": "2dd38247-8d00-466b-c749-1a569210f997"
      },
      "source": [
        "log_softmax = LogSoftmax()\n",
        "x = torch.rand([1, 1], requires_grad=True).double()\n",
        "assert gradcheck(log_softmax, x, eps=1e-3, atol=1e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Sn2M_Q086k2"
      },
      "source": [
        "### 2.4 Dropout\n",
        "Реализуйте слой Dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCLECy1y86k3"
      },
      "source": [
        "class MyDropout(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, inp, p):\n",
        "        ctx.save_for_backward(inp)\n",
        "        binomial = torch.distributions.binomial.Binomial(probs=1-self.p)\n",
        "        return inp * binomial.sample(inp.size()) * (1.0/(1-self.p))\n",
        "        \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, = ctx.saved_tensors\n",
        "        result = input\n",
        "        return grad_output.matmul(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS5z_mth1ZAZ"
      },
      "source": [
        "class Dropout(nn.Module):\n",
        "    def __init__(self, p):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        \n",
        "    def forward(self, input):\n",
        "      return MyDropout.apply(input, p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyPPzkCI86k7"
      },
      "source": [
        "### 2.5 CrossEntropy\n",
        "\n",
        "Реализуйте функцию потерь - кроссэнтропию. В разделе 2.3 приведены полезные формулы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KotXWXnT3-j5"
      },
      "source": [
        "class CrossEntropy(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, activations, target):\n",
        "        ctx.save_for_backward(activations, target)\n",
        "        log_prob = - MyLogSoftmax.apply(activations)\n",
        "        loss = torch.gather(log_prob, 1, target.unsqueeze(1)).view(-1)\n",
        "        loss = loss.mean()\n",
        "        return loss\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        activations, target,  = ctx.saved_tensors\n",
        "        p = torch.nn.functional.softmax(activations)\n",
        "\n",
        "        t = torch.zeros(p.size(), device=activations.device)\n",
        "        for n in range(t.size(0)):\n",
        "            t[n][target.data[n]] = 1\n",
        "        \n",
        "        grad_activations = (p - t) / t.shape[0]\n",
        "        grad_activations = grad_output * grad_activations\n",
        "        return grad_activations, None\n",
        "\n",
        "class CrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, activations, target):\n",
        "        return CrossEntropy.apply(activations, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFNcOfVNesCC"
      },
      "source": [
        "Проверка градиентов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXQr7sllKi8k",
        "outputId": "4f14c6c6-ff3d-4165-d1c9-fc140e7296e8"
      },
      "source": [
        "ce = CrossEntropyLoss()\n",
        "x = torch.rand([16, 10], requires_grad=True).double()\n",
        "target = torch.arange(16) % 10\n",
        "assert gradcheck(ce, inputs=(x, target), eps=1e-4, atol=1e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  del sys.path[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmzKDpyE86lg"
      },
      "source": [
        "## 3. Сборка и обучение нейронной сети (5 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPqlZfj_86lg"
      },
      "source": [
        "Реализуйте произвольную нейросеть, состоящую из ваших блоков. Она должна состоять из нескольких полносвязных слоев."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkESXVD87sM8"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, input_size=28*28, hidden_layers_size=32, num_layers=5,\n",
        "                 num_classes=10):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        \n",
        "        layers = []\n",
        "        layers.append(torch.nn.Flatten())\n",
        "        for i in range(num_layers + 1):\n",
        "            if i == 0:\n",
        "                layers.append(Dense(self.input_size, hidden_layers_size))\n",
        "                layers.append(ReLU())\n",
        "            else:\n",
        "                if i == num_layers:\n",
        "                    layers.append(Dense(hidden_layers_size, num_classes))\n",
        "                    layers.append(torch.nn.Softmax(dim=1))\n",
        "                else:\n",
        "                    layers.append(Dense(hidden_layers_size, hidden_layers_size))\n",
        "                    layers.append(ReLU())\n",
        "        self.layers = torch.nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        model = nn.Sequential(*self.layers)\n",
        "        y = model(inp)\n",
        "        return y\n",
        "        \n",
        "    def predict(self, inp):\n",
        "        y = self.forward(inp)\n",
        "        y = torch.argmax(y, dim=1)\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj-eEvMFfbSo"
      },
      "source": [
        "Ниже приведены функции, реализующие обучение нейронной сети. В данном задании их предлагается просто переиспользовать. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDhcoCB4OpXE"
      },
      "source": [
        "class EmptyContext:\n",
        "    def __enter__(self):\n",
        "        pass\n",
        "    \n",
        "    def __exit__(self, *args):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AsWblqIOquI"
      },
      "source": [
        "# accuract metric for our classififcation\n",
        "def accuracy(model_answers, labels):\n",
        "  return torch.mean((model_answers == labels).float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy33FHuv_Us-"
      },
      "source": [
        "def perform_epoch(model, loader, criterion, \n",
        "                optimizer=None, device=None):\n",
        "    is_train = optimizer is not None\n",
        "    model = model.to(device)\n",
        "    if is_train:  \n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    total_n = 0\n",
        "    with EmptyContext() if is_train else torch.no_grad():\n",
        "        for batch_data, batch_answers in loader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_answers = batch_answers.to(device)          \n",
        "            model_answers = model(batch_data)\n",
        "            model_prediction = model.predict(batch_data)\n",
        "            new_loss = criterion(model_answers, batch_answers)\n",
        "            if is_train:\n",
        "              optimizer.zero_grad()\n",
        "              new_loss.backward()\n",
        "              optimizer.step() \n",
        "\n",
        "            one_batch_loss = float(criterion(model_answers, batch_answers))\n",
        "            one_batch_acc = accuracy(model_prediction, batch_answers)\n",
        "            \n",
        "            total_loss += one_batch_loss\n",
        "            total_acc += one_batch_acc\n",
        "            total_n += 1 \n",
        "    return (total_loss / total_n, total_acc / total_n) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtamPEJZgOY5"
      },
      "source": [
        "Теперь обучим нашу нейронную сеть. В данном разделе будем использовать оптимизатор `Adam` с дефолтными пораметрами."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEcyUJJI_aAn"
      },
      "source": [
        "model = Network()\n",
        "optimizer = Adam(model.parameters())\n",
        "criterion = CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Imxjhjh86Yuo",
        "outputId": "0a76acc0-f151-43a0-a493-228d7b537e0d"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (layers): ModuleList(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Dense()\n",
            "    (2): ReLU()\n",
            "    (3): Dense()\n",
            "    (4): ReLU()\n",
            "    (5): Dense()\n",
            "    (6): ReLU()\n",
            "    (7): Dense()\n",
            "    (8): ReLU()\n",
            "    (9): Dense()\n",
            "    (10): ReLU()\n",
            "    (11): Dense()\n",
            "    (12): Softmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBADDSi0_jx7",
        "outputId": "855a71e3-4a23-4d1c-ccf9-9d4753865a4a"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(10):\n",
        "    loss, acc = perform_epoch(model, train_dataloader, criterion, \n",
        "                                optimizer=optimizer, device=device)\n",
        "    print(f\"Epoch - {epoch} : train loss {loss}, train accuracy {acc}\")\n",
        "    print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch - 0 : train loss 2.2187248935699464, train accuracy 0.2357858419418335\n",
            "Current learning rate: 0.001\n",
            "Epoch - 1 : train loss 2.0977883914113047, train accuracy 0.36315426230430603\n",
            "Current learning rate: 0.001\n",
            "Epoch - 2 : train loss 2.09760873028636, train accuracy 0.36348867416381836\n",
            "Current learning rate: 0.001\n",
            "Epoch - 3 : train loss 2.114161060422659, train accuracy 0.3466717004776001\n",
            "Current learning rate: 0.001\n",
            "Epoch - 4 : train loss 2.143148218214512, train accuracy 0.31793808937072754\n",
            "Current learning rate: 0.001\n",
            "Epoch - 5 : train loss 2.095349650233984, train accuracy 0.3656213581562042\n",
            "Current learning rate: 0.001\n",
            "Epoch - 6 : train loss 2.091664196640253, train accuracy 0.36947235465049744\n",
            "Current learning rate: 0.001\n",
            "Epoch - 7 : train loss 2.2329826738238334, train accuracy 0.22815336287021637\n",
            "Current learning rate: 0.001\n",
            "Epoch - 8 : train loss 2.2589062004089357, train accuracy 0.2022363841533661\n",
            "Current learning rate: 0.001\n",
            "Epoch - 9 : train loss 2.2810517975687983, train accuracy 0.1801033914089203\n",
            "Current learning rate: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxrvNtm3YSJn",
        "outputId": "aa650b86-ee46-48ec-b1fe-605182cc92fb"
      },
      "source": [
        "print(f\"Epoch - {epoch} : loss {loss}, accuracy {acc}\")\n",
        "print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch - 9 : loss 2.2810517975687983, accuracy 0.1801033914089203\n",
            "Current learning rate: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS41CQj0Y6Q8"
      },
      "source": [
        "Дальше:\n",
        "- Проведите эксперементы с числом слоев. \n",
        "- Постройте графики зависимости качества на трейне и нa тесте от числа слоев. Для получения статистически значимых результатов повторите эксперемени несколько раз.\n",
        "- Сделайте выводы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38_OmCEmg7ad",
        "outputId": "fca71d75-8eec-458b-8c03-5340442df9d4"
      },
      "source": [
        "layers = np.arange(1, 7)\n",
        "losses_train = []\n",
        "losses_test = []\n",
        "accs_train = []\n",
        "accs_test = []\n",
        "for num_layers in layers:\n",
        "    loss_train = []\n",
        "    acc_train = []\n",
        "    loss_test = []\n",
        "    acc_test = []\n",
        "    for repeat in range(5):\n",
        "        model = Network(num_layers=num_layers)\n",
        "        optimizer = Adam(model.parameters())\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model.to(device)\n",
        "\n",
        "        for epoch in range(10):\n",
        "            loss_train_val, acc_train_val = perform_epoch(model, train_dataloader, criterion, \n",
        "                                        optimizer=optimizer, device=device)\n",
        "            \n",
        "            loss_test_val, acc_test_val = perform_epoch(model, test_dataloader, criterion, device=device)\n",
        "            \n",
        "            print(f\"Repeat - {repeat}, Number of layers - {num_layers}, Epoch - {epoch}\")\n",
        "            print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "            print(f\"Train: loss {loss_train_val}, accuracy {acc_train_val}\")\n",
        "            print(f\"Test: loss {loss_test_val}, accuracy {acc_test_val}\")\n",
        "\n",
        "        print('----------------------')\n",
        "        loss_train.append(loss_train_val)\n",
        "        acc_train.append(acc_train_val.item())\n",
        "        loss_test.append(loss_test_val)\n",
        "        acc_test.append(acc_test_val.item())\n",
        "    losses_train.append(np.mean(loss_train))\n",
        "    accs_train.append(np.mean(acc_train))\n",
        "    losses_test.append(np.mean(loss_test))\n",
        "    accs_test.append(np.mean(acc_test))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repeat - 0, Number of layers - 1, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9345741205215454, accuracy 0.5260336399078369\n",
            "Test: loss 1.988910278280278, accuracy 0.4719637334346771\n",
            "Repeat - 0, Number of layers - 1, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8650023164451122, accuracy 0.5959835648536682\n",
            "Test: loss 1.8694840371161923, accuracy 0.5912047028541565\n",
            "Repeat - 0, Number of layers - 1, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8717257034480572, accuracy 0.589333176612854\n",
            "Test: loss 1.8459708694217802, accuracy 0.6151425242424011\n",
            "Repeat - 0, Number of layers - 1, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8518329998552798, accuracy 0.6092695593833923\n",
            "Test: loss 1.8572538999960697, accuracy 0.6037982702255249\n",
            "Repeat - 0, Number of layers - 1, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8520728167593479, accuracy 0.6090346574783325\n",
            "Test: loss 1.8436087083959507, accuracy 0.6174911260604858\n",
            "Repeat - 0, Number of layers - 1, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8604730790555477, accuracy 0.6006839275360107\n",
            "Test: loss 1.8609008336889332, accuracy 0.6002004742622375\n",
            "Repeat - 0, Number of layers - 1, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8520480106770991, accuracy 0.6091349124908447\n",
            "Test: loss 1.8431253579781688, accuracy 0.618040919303894\n",
            "Repeat - 0, Number of layers - 1, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8460023641586303, accuracy 0.6150846481323242\n",
            "Test: loss 1.8498815794934755, accuracy 0.6112943887710571\n",
            "Repeat - 0, Number of layers - 1, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8307798184752464, accuracy 0.6303541660308838\n",
            "Test: loss 1.8893947099102313, accuracy 0.5716647505760193\n",
            "Repeat - 0, Number of layers - 1, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.818181424409151, accuracy 0.6429225206375122\n",
            "Test: loss 1.782502975957147, accuracy 0.6786600351333618\n",
            "----------------------\n",
            "Repeat - 1, Number of layers - 1, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.182168691337109, accuracy 0.27863627672195435\n",
            "Test: loss 2.18756599851634, accuracy 0.273662269115448\n",
            "Repeat - 1, Number of layers - 1, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1646409074366093, accuracy 0.2963367998600006\n",
            "Test: loss 2.2011578924831063, accuracy 0.26001912355422974\n",
            "Repeat - 1, Number of layers - 1, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1851055280268192, accuracy 0.2759856581687927\n",
            "Test: loss 2.180550928058653, accuracy 0.2805589437484741\n",
            "Repeat - 1, Number of layers - 1, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1711505907177924, accuracy 0.28997018933296204\n",
            "Test: loss 2.1663696235087677, accuracy 0.2948014736175537\n",
            "Repeat - 1, Number of layers - 1, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1715278423428535, accuracy 0.28961947560310364\n",
            "Test: loss 2.173874511354152, accuracy 0.2872553765773773\n",
            "Repeat - 1, Number of layers - 1, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.167833700954914, accuracy 0.2933031916618347\n",
            "Test: loss 2.16819290546463, accuracy 0.29295241832733154\n",
            "Repeat - 1, Number of layers - 1, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.17004623106122, accuracy 0.2910861670970917\n",
            "Test: loss 2.166539606483265, accuracy 0.2946016490459442\n",
            "Repeat - 1, Number of layers - 1, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1676612199544905, accuracy 0.29348668456077576\n",
            "Test: loss 2.166237649352833, accuracy 0.2949015498161316\n",
            "Repeat - 1, Number of layers - 1, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1699037859737875, accuracy 0.29123619198799133\n",
            "Test: loss 2.174534250592542, accuracy 0.2866057753562927\n",
            "Repeat - 1, Number of layers - 1, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.155150816500187, accuracy 0.3061204254627228\n",
            "Test: loss 2.09601307612309, accuracy 0.365116149187088\n",
            "----------------------\n",
            "Repeat - 2, Number of layers - 1, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9818909440636634, accuracy 0.47884881496429443\n",
            "Test: loss 1.8809204828971509, accuracy 0.5800600051879883\n",
            "Repeat - 2, Number of layers - 1, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8914153246581555, accuracy 0.569499135017395\n",
            "Test: loss 1.891967210097649, accuracy 0.5688658952713013\n",
            "Repeat - 2, Number of layers - 1, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8510035000741483, accuracy 0.6099515557289124\n",
            "Test: loss 1.813372613071859, accuracy 0.6476258635520935\n",
            "Repeat - 2, Number of layers - 1, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8360991457104683, accuracy 0.6249039173126221\n",
            "Test: loss 1.865064277999226, accuracy 0.5958525538444519\n",
            "Repeat - 2, Number of layers - 1, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8363977575302124, accuracy 0.624636173248291\n",
            "Test: loss 1.851967761362868, accuracy 0.609195351600647\n",
            "Repeat - 2, Number of layers - 1, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8298566276431083, accuracy 0.6311205625534058\n",
            "Test: loss 1.8158432740321582, accuracy 0.6453273296356201\n",
            "Repeat - 2, Number of layers - 1, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8160224332511425, accuracy 0.6449715495109558\n",
            "Test: loss 1.8055461144697542, accuracy 0.655572235584259\n",
            "Repeat - 2, Number of layers - 1, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.813863404393196, accuracy 0.647139310836792\n",
            "Test: loss 1.8082874033940786, accuracy 0.6527732610702515\n",
            "Repeat - 2, Number of layers - 1, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.792239544659853, accuracy 0.6688247323036194\n",
            "Test: loss 1.8109672087422017, accuracy 0.6502249836921692\n",
            "Repeat - 2, Number of layers - 1, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8093628578484058, accuracy 0.6517561674118042\n",
            "Test: loss 1.816551405093123, accuracy 0.6444778442382812\n",
            "----------------------\n",
            "Repeat - 3, Number of layers - 1, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9643239299952984, accuracy 0.4960998594760895\n",
            "Test: loss 1.9763564321769589, accuracy 0.4844575822353363\n",
            "Repeat - 3, Number of layers - 1, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9460677958130836, accuracy 0.5148990154266357\n",
            "Test: loss 1.9407662551799814, accuracy 0.5201401114463806\n",
            "Repeat - 3, Number of layers - 1, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9515176789760589, accuracy 0.5095981955528259\n",
            "Test: loss 1.9446296237934118, accuracy 0.5165417790412903\n",
            "Repeat - 3, Number of layers - 1, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9488882664442062, accuracy 0.5121827125549316\n",
            "Test: loss 1.9561693652280268, accuracy 0.5048475861549377\n",
            "Repeat - 3, Number of layers - 1, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9510381355881692, accuracy 0.5100153684616089\n",
            "Test: loss 1.9680736922788835, accuracy 0.4929533302783966\n",
            "Repeat - 3, Number of layers - 1, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9542442198097705, accuracy 0.5068166851997375\n",
            "Test: loss 1.952843236422789, accuracy 0.5082460045814514\n",
            "Repeat - 3, Number of layers - 1, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9388155145049095, accuracy 0.5222483277320862\n",
            "Test: loss 1.943281213204185, accuracy 0.5177913308143616\n",
            "Repeat - 3, Number of layers - 1, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9352907183170318, accuracy 0.525848388671875\n",
            "Test: loss 1.9508640113799112, accuracy 0.5102449655532837\n",
            "Repeat - 3, Number of layers - 1, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9353080278038979, accuracy 0.5257801413536072\n",
            "Test: loss 1.9424136932464553, accuracy 0.5186408162117004\n",
            "Repeat - 3, Number of layers - 1, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9450177320837974, accuracy 0.5161148309707642\n",
            "Test: loss 1.953506228984564, accuracy 0.5075963139533997\n",
            "----------------------\n",
            "Repeat - 4, Number of layers - 1, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9597895039916038, accuracy 0.5007303953170776\n",
            "Test: loss 1.9381399580027567, accuracy 0.5226387977600098\n",
            "Repeat - 4, Number of layers - 1, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9375358624756336, accuracy 0.5234814882278442\n",
            "Test: loss 1.923783874404484, accuracy 0.5371816158294678\n",
            "Repeat - 4, Number of layers - 1, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9162141103744508, accuracy 0.5448651909828186\n",
            "Test: loss 1.9047247589617475, accuracy 0.556372344493866\n",
            "Repeat - 4, Number of layers - 1, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8884177075028419, accuracy 0.5725977420806885\n",
            "Test: loss 1.893239817519238, accuracy 0.5678662061691284\n",
            "Repeat - 4, Number of layers - 1, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.884161705583334, accuracy 0.5769829750061035\n",
            "Test: loss 1.8820806141199915, accuracy 0.5788110494613647\n",
            "Repeat - 4, Number of layers - 1, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.891850915491581, accuracy 0.5692490935325623\n",
            "Test: loss 1.8999570692139587, accuracy 0.5610700249671936\n",
            "Repeat - 4, Number of layers - 1, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8899125884473325, accuracy 0.5711320638656616\n",
            "Test: loss 1.873338576735764, accuracy 0.5877568125724792\n",
            "Repeat - 4, Number of layers - 1, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8725105131864548, accuracy 0.5885827541351318\n",
            "Test: loss 1.8640463684631072, accuracy 0.5970519781112671\n",
            "Repeat - 4, Number of layers - 1, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8700063507258893, accuracy 0.5911332368850708\n",
            "Test: loss 1.8734878021022905, accuracy 0.5877062678337097\n",
            "Repeat - 4, Number of layers - 1, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.873484001636505, accuracy 0.5876497626304626\n",
            "Test: loss 1.8693082691847474, accuracy 0.5918543338775635\n",
            "----------------------\n",
            "Repeat - 0, Number of layers - 2, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7458573042750358, accuracy 0.7151966691017151\n",
            "Test: loss 1.7988356258915639, accuracy 0.6618687510490417\n",
            "Repeat - 0, Number of layers - 2, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7243269771635532, accuracy 0.7363492846488953\n",
            "Test: loss 1.7311038458007744, accuracy 0.730134129524231\n",
            "Repeat - 0, Number of layers - 2, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7326875798106194, accuracy 0.7283151745796204\n",
            "Test: loss 1.7326921137019076, accuracy 0.7287346720695496\n",
            "Repeat - 0, Number of layers - 2, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7413177529871464, accuracy 0.7198135852813721\n",
            "Test: loss 1.7031514769730003, accuracy 0.7580199837684631\n",
            "Repeat - 0, Number of layers - 2, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7222760077416897, accuracy 0.7388133406639099\n",
            "Test: loss 1.6921591408427865, accuracy 0.7690643072128296\n",
            "Repeat - 0, Number of layers - 2, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7149688349068164, accuracy 0.7461144924163818\n",
            "Test: loss 1.7043958623548676, accuracy 0.7565206289291382\n",
            "Repeat - 0, Number of layers - 2, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7260042463839054, accuracy 0.7350813746452332\n",
            "Test: loss 1.7111419444677534, accuracy 0.7497239112854004\n",
            "Repeat - 0, Number of layers - 2, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.721981921941042, accuracy 0.7389827966690063\n",
            "Test: loss 1.7089142534865074, accuracy 0.7521729469299316\n",
            "Repeat - 0, Number of layers - 2, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7107682362496852, accuracy 0.750331699848175\n",
            "Test: loss 1.7282730548278145, accuracy 0.7327326536178589\n",
            "Repeat - 0, Number of layers - 2, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7155278568267822, accuracy 0.7455652356147766\n",
            "Test: loss 1.6928574693971488, accuracy 0.7682146430015564\n",
            "----------------------\n",
            "Repeat - 1, Number of layers - 2, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9365665285885334, accuracy 0.5244655609130859\n",
            "Test: loss 1.9166810315945695, accuracy 0.5442285537719727\n",
            "Repeat - 1, Number of layers - 2, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9267133841216564, accuracy 0.5341149568557739\n",
            "Test: loss 1.9213406599979888, accuracy 0.5396804809570312\n",
            "Repeat - 1, Number of layers - 2, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9224790388941764, accuracy 0.5384802222251892\n",
            "Test: loss 1.9956192509166484, accuracy 0.46531733870506287\n",
            "Repeat - 1, Number of layers - 2, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9250938033759595, accuracy 0.5357146859169006\n",
            "Test: loss 1.9015109640904988, accuracy 0.5594205856323242\n",
            "Repeat - 1, Number of layers - 2, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.891674067914486, accuracy 0.5693322420120239\n",
            "Test: loss 1.902509517576741, accuracy 0.5585207939147949\n",
            "Repeat - 1, Number of layers - 2, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8994351867437362, accuracy 0.5614829659461975\n",
            "Test: loss 1.8388747992007985, accuracy 0.6214894652366638\n",
            "Repeat - 1, Number of layers - 2, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.824789688706398, accuracy 0.636271059513092\n",
            "Test: loss 1.8025258963612065, accuracy 0.6583711504936218\n",
            "Repeat - 1, Number of layers - 2, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8168951627016068, accuracy 0.6441212892532349\n",
            "Test: loss 1.8635896892919355, accuracy 0.5975517630577087\n",
            "Repeat - 1, Number of layers - 2, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8316715439856053, accuracy 0.6293871998786926\n",
            "Test: loss 1.8069275957176174, accuracy 0.654122531414032\n",
            "Repeat - 1, Number of layers - 2, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8210675565600396, accuracy 0.6399887204170227\n",
            "Test: loss 1.8094447908372893, accuracy 0.6516740322113037\n",
            "----------------------\n",
            "Repeat - 2, Number of layers - 2, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7171850758492946, accuracy 0.7439800500869751\n",
            "Test: loss 1.6921298741579414, accuracy 0.7684646248817444\n",
            "Repeat - 2, Number of layers - 2, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.6932896282076835, accuracy 0.7674682140350342\n",
            "Test: loss 1.7390323473417062, accuracy 0.7216883301734924\n",
            "Repeat - 2, Number of layers - 2, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.6999080524146557, accuracy 0.7609843015670776\n",
            "Test: loss 1.7917821913108654, accuracy 0.6688152551651001\n",
            "Repeat - 2, Number of layers - 2, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.687914773195982, accuracy 0.772901177406311\n",
            "Test: loss 1.7023182381754336, accuracy 0.7586194276809692\n",
            "Repeat - 2, Number of layers - 2, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.6859637120366096, accuracy 0.7750017642974854\n",
            "Test: loss 1.6869569759497578, accuracy 0.7740119099617004\n",
            "Repeat - 2, Number of layers - 2, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.691625933855772, accuracy 0.769351065158844\n",
            "Test: loss 1.7130804438998495, accuracy 0.7482249140739441\n",
            "Repeat - 2, Number of layers - 2, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7017411815226078, accuracy 0.759215235710144\n",
            "Test: loss 1.7121929346710845, accuracy 0.748874843120575\n",
            "Repeat - 2, Number of layers - 2, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.703955890417099, accuracy 0.7570814490318298\n",
            "Test: loss 1.6761658802918944, accuracy 0.7849059104919434\n",
            "Repeat - 2, Number of layers - 2, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.6777870702147484, accuracy 0.7832350134849548\n",
            "Test: loss 1.746271605076997, accuracy 0.7148914933204651\n",
            "Repeat - 2, Number of layers - 2, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7319819634258746, accuracy 0.7291138172149658\n",
            "Test: loss 1.7253104512659327, accuracy 0.7358810305595398\n",
            "----------------------\n",
            "Repeat - 3, Number of layers - 2, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7538837169110775, accuracy 0.7072120308876038\n",
            "Test: loss 1.7403373130257878, accuracy 0.7211387753486633\n",
            "Repeat - 3, Number of layers - 2, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7131481750011444, accuracy 0.7476828098297119\n",
            "Test: loss 1.6984684091994073, accuracy 0.7624677419662476\n",
            "Repeat - 3, Number of layers - 2, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7095319128334523, accuracy 0.7514336109161377\n",
            "Test: loss 1.7006977565999868, accuracy 0.7602189183235168\n",
            "Repeat - 3, Number of layers - 2, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.6911322287023067, accuracy 0.7696839570999146\n",
            "Test: loss 1.7426126196287919, accuracy 0.7182900905609131\n",
            "Repeat - 3, Number of layers - 2, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.725366380751133, accuracy 0.735663652420044\n",
            "Test: loss 1.842355422887845, accuracy 0.6185410618782043\n",
            "Repeat - 3, Number of layers - 2, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7078276754915713, accuracy 0.7532484531402588\n",
            "Test: loss 1.724357678793717, accuracy 0.7367302775382996\n",
            "Repeat - 3, Number of layers - 2, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7362892768681049, accuracy 0.7247458696365356\n",
            "Test: loss 1.7681464174876864, accuracy 0.6929032206535339\n",
            "Repeat - 3, Number of layers - 2, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7254319387078285, accuracy 0.735662579536438\n",
            "Test: loss 1.735147081572434, accuracy 0.7259860634803772\n",
            "Repeat - 3, Number of layers - 2, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7082817457914352, accuracy 0.7528154253959656\n",
            "Test: loss 1.6887688248947463, accuracy 0.7724623084068298\n",
            "Repeat - 3, Number of layers - 2, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.703454201132059, accuracy 0.7576656937599182\n",
            "Test: loss 1.7868578889738138, accuracy 0.6744127869606018\n",
            "----------------------\n",
            "Repeat - 4, Number of layers - 2, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8229279902279376, accuracy 0.6380723118782043\n",
            "Test: loss 1.785833898155407, accuracy 0.6748624444007874\n",
            "Repeat - 4, Number of layers - 2, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7965394997894764, accuracy 0.6643410921096802\n",
            "Test: loss 1.8694899275206376, accuracy 0.5910550355911255\n",
            "Repeat - 4, Number of layers - 2, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8036795176565648, accuracy 0.6573238372802734\n",
            "Test: loss 1.7895418223233774, accuracy 0.6716638803482056\n",
            "Repeat - 4, Number of layers - 2, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.79162090626359, accuracy 0.6694240570068359\n",
            "Test: loss 1.7997991915406852, accuracy 0.6612687706947327\n",
            "Repeat - 4, Number of layers - 2, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7964189179837704, accuracy 0.6646068692207336\n",
            "Test: loss 1.7775996058419727, accuracy 0.683458149433136\n",
            "Repeat - 4, Number of layers - 2, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8137835963368416, accuracy 0.6472206711769104\n",
            "Test: loss 1.8512963506950253, accuracy 0.609795331954956\n",
            "Repeat - 4, Number of layers - 2, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8176285764575004, accuracy 0.6434868574142456\n",
            "Test: loss 1.8267675715050418, accuracy 0.6343327760696411\n",
            "Repeat - 4, Number of layers - 2, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.799275081217289, accuracy 0.6618415117263794\n",
            "Test: loss 1.7767940879404276, accuracy 0.684457540512085\n",
            "Repeat - 4, Number of layers - 2, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8025451722443104, accuracy 0.6585902571678162\n",
            "Test: loss 1.7861348547856848, accuracy 0.6750123500823975\n",
            "Repeat - 4, Number of layers - 2, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.809167460232973, accuracy 0.6519719362258911\n",
            "Test: loss 1.8006940131065907, accuracy 0.6604695916175842\n",
            "----------------------\n",
            "Repeat - 0, Number of layers - 3, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.957761644244194, accuracy 0.5027292370796204\n",
            "Test: loss 1.8955579542148595, accuracy 0.5649176836013794\n",
            "Repeat - 0, Number of layers - 3, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.913050469994545, accuracy 0.5477153062820435\n",
            "Test: loss 1.9040078745074178, accuracy 0.556522011756897\n",
            "Repeat - 0, Number of layers - 3, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8908603816628455, accuracy 0.5701164603233337\n",
            "Test: loss 1.9554487750984204, accuracy 0.5055475234985352\n",
            "Repeat - 0, Number of layers - 3, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9188978227078914, accuracy 0.5420951247215271\n",
            "Test: loss 1.9733133539445755, accuracy 0.4879060387611389\n",
            "Repeat - 0, Number of layers - 3, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9558223442733287, accuracy 0.505252480506897\n",
            "Test: loss 1.949345902345706, accuracy 0.5117942094802856\n",
            "Repeat - 0, Number of layers - 3, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.915501702040434, accuracy 0.5455814003944397\n",
            "Test: loss 1.9147215011059076, accuracy 0.5464770197868347\n",
            "Repeat - 0, Number of layers - 3, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9455673388242722, accuracy 0.5155470967292786\n",
            "Test: loss 1.8738284577494082, accuracy 0.587256908416748\n",
            "Repeat - 0, Number of layers - 3, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9078569958209992, accuracy 0.5532488822937012\n",
            "Test: loss 1.9108871332351594, accuracy 0.5502747893333435\n",
            "Repeat - 0, Number of layers - 3, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9215530120134354, accuracy 0.5395995378494263\n",
            "Test: loss 1.9137598193925003, accuracy 0.5473765730857849\n",
            "Repeat - 0, Number of layers - 3, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9291392824351787, accuracy 0.5319647789001465\n",
            "Test: loss 1.8751537299406404, accuracy 0.5859574675559998\n",
            "----------------------\n",
            "Repeat - 1, Number of layers - 3, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8707799444794655, accuracy 0.5896152853965759\n",
            "Test: loss 1.8614690323462193, accuracy 0.5995503067970276\n",
            "Repeat - 1, Number of layers - 3, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8362415569126607, accuracy 0.6247369050979614\n",
            "Test: loss 1.8748166230486252, accuracy 0.5851577520370483\n",
            "Repeat - 1, Number of layers - 3, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7951792243421079, accuracy 0.6657578349113464\n",
            "Test: loss 1.8145375537729336, accuracy 0.6465268731117249\n",
            "Repeat - 1, Number of layers - 3, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8565223867595195, accuracy 0.6045165657997131\n",
            "Test: loss 1.7891846226787995, accuracy 0.6720138788223267\n",
            "Repeat - 1, Number of layers - 3, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8526631864905356, accuracy 0.6084195375442505\n",
            "Test: loss 1.8628932721968712, accuracy 0.5981511473655701\n",
            "Repeat - 1, Number of layers - 3, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8078109540045262, accuracy 0.653286874294281\n",
            "Test: loss 1.8460633826220054, accuracy 0.6149423122406006\n",
            "Repeat - 1, Number of layers - 3, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7919505705237389, accuracy 0.6691740155220032\n",
            "Test: loss 1.7762644337392461, accuracy 0.6849073171615601\n",
            "Repeat - 1, Number of layers - 3, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7840939990580083, accuracy 0.6769915223121643\n",
            "Test: loss 1.7683687783788884, accuracy 0.692653238773346\n",
            "Repeat - 1, Number of layers - 3, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7714628989696504, accuracy 0.6896756887435913\n",
            "Test: loss 1.7627732137153889, accuracy 0.6983005404472351\n",
            "Repeat - 1, Number of layers - 3, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7722943057715892, accuracy 0.6888092756271362\n",
            "Test: loss 1.8074416231358426, accuracy 0.653623104095459\n",
            "----------------------\n",
            "Repeat - 2, Number of layers - 3, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9482705935537814, accuracy 0.5118350386619568\n",
            "Test: loss 2.009578194754056, accuracy 0.4516235589981079\n",
            "Repeat - 2, Number of layers - 3, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9864906546473504, accuracy 0.4745209515094757\n",
            "Test: loss 2.0390152941936854, accuracy 0.42193806171417236\n",
            "Repeat - 2, Number of layers - 3, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.011935486584902, accuracy 0.4491199254989624\n",
            "Test: loss 2.0070559215331185, accuracy 0.4540225565433502\n",
            "Repeat - 2, Number of layers - 3, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.017686980456114, accuracy 0.44335299730300903\n",
            "Test: loss 2.0027736857913245, accuracy 0.4583704471588135\n",
            "Repeat - 2, Number of layers - 3, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0126638413071634, accuracy 0.44833698868751526\n",
            "Test: loss 2.0213491711123237, accuracy 0.4396296739578247\n",
            "Repeat - 2, Number of layers - 3, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0112452056109906, accuracy 0.44985368847846985\n",
            "Test: loss 2.0024419910368, accuracy 0.4587703049182892\n",
            "Repeat - 2, Number of layers - 3, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0154614253044127, accuracy 0.44565412402153015\n",
            "Test: loss 2.002631481798335, accuracy 0.4585707187652588\n",
            "Repeat - 2, Number of layers - 3, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0058399585187434, accuracy 0.45525380969047546\n",
            "Test: loss 1.9989863230191964, accuracy 0.4621187150478363\n",
            "Repeat - 2, Number of layers - 3, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0281052822470667, accuracy 0.4330388903617859\n",
            "Test: loss 2.0902807276466975, accuracy 0.370863139629364\n",
            "Repeat - 2, Number of layers - 3, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.088616872817278, accuracy 0.3725375831127167\n",
            "Test: loss 2.004809782780271, accuracy 0.4562717080116272\n",
            "----------------------\n",
            "Repeat - 3, Number of layers - 3, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9013639090955257, accuracy 0.5585131645202637\n",
            "Test: loss 1.8469228038544776, accuracy 0.6138932704925537\n",
            "Repeat - 3, Number of layers - 3, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8484798626303673, accuracy 0.6123854517936707\n",
            "Test: loss 1.8415412788448304, accuracy 0.6197901964187622\n",
            "Repeat - 3, Number of layers - 3, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.850052024692297, accuracy 0.6108684539794922\n",
            "Test: loss 1.8318192819426622, accuracy 0.6291355490684509\n",
            "Repeat - 3, Number of layers - 3, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8631352669894696, accuracy 0.5978997945785522\n",
            "Test: loss 1.8638958911309536, accuracy 0.5971017479896545\n",
            "Repeat - 3, Number of layers - 3, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8465823401510715, accuracy 0.6145022511482239\n",
            "Test: loss 1.8369869208228642, accuracy 0.6241877675056458\n",
            "Repeat - 3, Number of layers - 3, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.845082625091076, accuracy 0.616052508354187\n",
            "Test: loss 1.838956586603282, accuracy 0.622139036655426\n",
            "Repeat - 3, Number of layers - 3, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8480144609808922, accuracy 0.6131187081336975\n",
            "Test: loss 1.83807946431047, accuracy 0.6229885816574097\n",
            "Repeat - 3, Number of layers - 3, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.845211576193571, accuracy 0.6159345507621765\n",
            "Test: loss 1.8892008361072912, accuracy 0.5719144940376282\n",
            "Repeat - 3, Number of layers - 3, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8542033827602864, accuracy 0.60685133934021\n",
            "Test: loss 1.8922014579601374, accuracy 0.5690162181854248\n",
            "Repeat - 3, Number of layers - 3, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8684945624172689, accuracy 0.5925683379173279\n",
            "Test: loss 1.8458478481158325, accuracy 0.615291953086853\n",
            "----------------------\n",
            "Repeat - 4, Number of layers - 3, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8130550893247128, accuracy 0.6472567915916443\n",
            "Test: loss 1.7789358318835005, accuracy 0.6812590956687927\n",
            "Repeat - 4, Number of layers - 3, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7804897246360778, accuracy 0.6804089546203613\n",
            "Test: loss 1.7938603350426303, accuracy 0.6674162745475769\n",
            "Repeat - 4, Number of layers - 3, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7968295721411705, accuracy 0.6641231179237366\n",
            "Test: loss 1.7876914623437792, accuracy 0.6733630299568176\n",
            "Repeat - 4, Number of layers - 3, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8182403800785543, accuracy 0.6427895426750183\n",
            "Test: loss 1.7873292123002449, accuracy 0.6737130284309387\n",
            "Repeat - 4, Number of layers - 3, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8212415196597576, accuracy 0.6398541927337646\n",
            "Test: loss 1.9254187895976442, accuracy 0.5355325937271118\n",
            "Repeat - 4, Number of layers - 3, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8164476653635502, accuracy 0.6447370052337646\n",
            "Test: loss 1.8516265827676524, accuracy 0.6094456911087036\n",
            "Repeat - 4, Number of layers - 3, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8289061159193516, accuracy 0.6321535706520081\n",
            "Test: loss 1.8015440897248138, accuracy 0.6595697999000549\n",
            "Repeat - 4, Number of layers - 3, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8094323045909404, accuracy 0.6516559720039368\n",
            "Test: loss 1.8841860705408557, accuracy 0.5769120454788208\n",
            "Repeat - 4, Number of layers - 3, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8103575165867805, accuracy 0.6508232355117798\n",
            "Test: loss 1.792402869936587, accuracy 0.6686654090881348\n",
            "Repeat - 4, Number of layers - 3, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8017097755670548, accuracy 0.6594237089157104\n",
            "Test: loss 1.777573265950719, accuracy 0.6836081147193909\n",
            "----------------------\n",
            "Repeat - 0, Number of layers - 4, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.2550216290354728, accuracy 0.20353713631629944\n",
            "Test: loss 2.2621727777921454, accuracy 0.19895021617412567\n",
            "Repeat - 0, Number of layers - 4, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.262569319218397, accuracy 0.19855301082134247\n",
            "Test: loss 2.261599142512103, accuracy 0.19954995810985565\n",
            "Repeat - 0, Number of layers - 4, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.262259305566549, accuracy 0.19886982440948486\n",
            "Test: loss 2.2623920535397852, accuracy 0.19875048100948334\n",
            "Repeat - 0, Number of layers - 4, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.32022887969017, accuracy 0.1409195512533188\n",
            "Test: loss 2.36115061575505, accuracy 0.10000008344650269\n",
            "Repeat - 0, Number of layers - 4, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3611506127119064, accuracy 0.10000148415565491\n",
            "Test: loss 2.3612006057327477, accuracy 0.09995020180940628\n",
            "Repeat - 0, Number of layers - 4, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.361150632917881, accuracy 0.10000132769346237\n",
            "Test: loss 2.361150631125363, accuracy 0.10000011324882507\n",
            "Repeat - 0, Number of layers - 4, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3611506323516367, accuracy 0.10000132769346237\n",
            "Test: loss 2.3611506332700576, accuracy 0.10000009834766388\n",
            "Repeat - 0, Number of layers - 4, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.361150634407997, accuracy 0.10000157356262207\n",
            "Test: loss 2.3611006568754274, accuracy 0.10005010664463043\n",
            "Repeat - 0, Number of layers - 4, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3611506339907646, accuracy 0.10000158846378326\n",
            "Test: loss 2.3612006085923407, accuracy 0.0999501571059227\n",
            "Repeat - 0, Number of layers - 4, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3611506326198577, accuracy 0.10000144690275192\n",
            "Test: loss 2.3610506836978393, accuracy 0.1001000702381134\n",
            "----------------------\n",
            "Repeat - 1, Number of layers - 4, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1018217877447607, accuracy 0.35697120428085327\n",
            "Test: loss 2.1794943718359745, accuracy 0.2817082107067108\n",
            "Repeat - 1, Number of layers - 4, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1780669955313208, accuracy 0.282853364944458\n",
            "Test: loss 2.1092364841672793, accuracy 0.3516727387905121\n",
            "Repeat - 1, Number of layers - 4, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.063890587747097, accuracy 0.39708584547042847\n",
            "Test: loss 2.010668574065819, accuracy 0.45042458176612854\n",
            "Repeat - 1, Number of layers - 4, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0119146578907965, accuracy 0.4492204785346985\n",
            "Test: loss 2.0215215134298963, accuracy 0.439929336309433\n",
            "Repeat - 1, Number of layers - 4, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.026697602748871, accuracy 0.4343033730983734\n",
            "Test: loss 1.972263328139035, accuracy 0.4887557029724121\n",
            "Repeat - 1, Number of layers - 4, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.021721811622381, accuracy 0.4394531548023224\n",
            "Test: loss 2.0151792807557594, accuracy 0.4461265504360199\n",
            "Repeat - 1, Number of layers - 4, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.028494882076979, accuracy 0.43258780241012573\n",
            "Test: loss 2.044598320256109, accuracy 0.4163912236690521\n",
            "Repeat - 1, Number of layers - 4, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0175468292236327, accuracy 0.44368624687194824\n",
            "Test: loss 2.0026523637986076, accuracy 0.45837026834487915\n",
            "Repeat - 1, Number of layers - 4, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9556556962132454, accuracy 0.5055005550384521\n",
            "Test: loss 1.9465981212513022, accuracy 0.514592707157135\n",
            "Repeat - 1, Number of layers - 4, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9460830362737178, accuracy 0.515032947063446\n",
            "Test: loss 1.9458679309313087, accuracy 0.5153423547744751\n",
            "----------------------\n",
            "Repeat - 2, Number of layers - 4, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9319504656791686, accuracy 0.5254459381103516\n",
            "Test: loss 1.9263076548097373, accuracy 0.5344825983047485\n",
            "Repeat - 2, Number of layers - 4, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.093534208446741, accuracy 0.36752161383628845\n",
            "Test: loss 2.1129624931172453, accuracy 0.3479245603084564\n",
            "Repeat - 2, Number of layers - 4, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1885963820815086, accuracy 0.27251800894737244\n",
            "Test: loss 2.2761196167930136, accuracy 0.1850574016571045\n",
            "Repeat - 2, Number of layers - 4, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1960224021077157, accuracy 0.2651190459728241\n",
            "Test: loss 2.1698739662699436, accuracy 0.29125353693962097\n",
            "Repeat - 2, Number of layers - 4, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1822435961663724, accuracy 0.27890148758888245\n",
            "Test: loss 2.2653415857226418, accuracy 0.1957520991563797\n",
            "Repeat - 2, Number of layers - 4, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.230441572338343, accuracy 0.23070205748081207\n",
            "Test: loss 2.1794103841552848, accuracy 0.28170812129974365\n",
            "Repeat - 2, Number of layers - 4, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.208996528238058, accuracy 0.2521527111530304\n",
            "Test: loss 2.2612496130112585, accuracy 0.1998998522758484\n",
            "Repeat - 2, Number of layers - 4, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3135333105623723, accuracy 0.14762012660503387\n",
            "Test: loss 2.261497915952817, accuracy 0.19964997470378876\n",
            "Repeat - 2, Number of layers - 4, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.262416722804308, accuracy 0.19873611629009247\n",
            "Test: loss 2.2620999705845093, accuracy 0.19905030727386475\n",
            "Repeat - 2, Number of layers - 4, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.2629690197110177, accuracy 0.19818659126758575\n",
            "Test: loss 2.266652905780157, accuracy 0.19450253248214722\n",
            "----------------------\n",
            "Repeat - 3, Number of layers - 4, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0798574156165124, accuracy 0.37946751713752747\n",
            "Test: loss 2.361142065214074, accuracy 0.10000009834766388\n",
            "Repeat - 3, Number of layers - 4, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.340576508194208, accuracy 0.1205344945192337\n",
            "Test: loss 2.263038639543296, accuracy 0.1981007605791092\n",
            "Repeat - 3, Number of layers - 4, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1625662940740584, accuracy 0.29916736483573914\n",
            "Test: loss 1.9545787821287872, accuracy 0.5071463584899902\n",
            "Repeat - 3, Number of layers - 4, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9348761386573314, accuracy 0.5265819430351257\n",
            "Test: loss 1.9280967224484262, accuracy 0.5335337519645691\n",
            "Repeat - 3, Number of layers - 4, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9281246653199196, accuracy 0.5328643918037415\n",
            "Test: loss 1.9339649273359079, accuracy 0.5235882997512817\n",
            "Repeat - 3, Number of layers - 4, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8850866507589816, accuracy 0.5760161876678467\n",
            "Test: loss 1.940123089249881, accuracy 0.5210398435592651\n",
            "Repeat - 3, Number of layers - 4, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8948777974545956, accuracy 0.5656997561454773\n",
            "Test: loss 1.856815710954223, accuracy 0.6036482453346252\n",
            "Repeat - 3, Number of layers - 4, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.863038199007511, accuracy 0.5980843305587769\n",
            "Test: loss 1.8442796458011268, accuracy 0.6176411509513855\n",
            "Repeat - 3, Number of layers - 4, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.840001254916191, accuracy 0.6212362051010132\n",
            "Test: loss 1.835914530854175, accuracy 0.6246877908706665\n",
            "Repeat - 3, Number of layers - 4, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8436895195543765, accuracy 0.6174525618553162\n",
            "Test: loss 1.8314436146880555, accuracy 0.6298354864120483\n",
            "----------------------\n",
            "Repeat - 4, Number of layers - 4, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8311529230475425, accuracy 0.6278372406959534\n",
            "Test: loss 1.7584670594428433, accuracy 0.7023482918739319\n",
            "Repeat - 4, Number of layers - 4, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7713587908446788, accuracy 0.6895596385002136\n",
            "Test: loss 1.7908721799435823, accuracy 0.6699651479721069\n",
            "Repeat - 4, Number of layers - 4, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8063191047906875, accuracy 0.6545516848564148\n",
            "Test: loss 1.8555640775760611, accuracy 0.6059972643852234\n",
            "Repeat - 4, Number of layers - 4, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8094058495759964, accuracy 0.6514893770217896\n",
            "Test: loss 1.7583468812873875, accuracy 0.7027478814125061\n",
            "Repeat - 4, Number of layers - 4, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7719547900259496, accuracy 0.6890777945518494\n",
            "Test: loss 1.7699828335548984, accuracy 0.6910046935081482\n",
            "Repeat - 4, Number of layers - 4, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.785856020092964, accuracy 0.6751740574836731\n",
            "Test: loss 1.8080546416264067, accuracy 0.6529240012168884\n",
            "Repeat - 4, Number of layers - 4, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7746425010859965, accuracy 0.6865267753601074\n",
            "Test: loss 1.8030957764592663, accuracy 0.6580209136009216\n",
            "Repeat - 4, Number of layers - 4, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.784744398355484, accuracy 0.6764423847198486\n",
            "Test: loss 1.8155957027889977, accuracy 0.6454271674156189\n",
            "Repeat - 4, Number of layers - 4, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8126535704135895, accuracy 0.6484553813934326\n",
            "Test: loss 1.7768492732627101, accuracy 0.6843071579933167\n",
            "Repeat - 4, Number of layers - 4, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.77504258492589, accuracy 0.6861095428466797\n",
            "Test: loss 1.7679365699973957, accuracy 0.6932032704353333\n",
            "----------------------\n",
            "Repeat - 0, Number of layers - 5, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1490423456132413, accuracy 0.30760112404823303\n",
            "Test: loss 2.2629615245372996, accuracy 0.1981509029865265\n",
            "Repeat - 0, Number of layers - 5, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.344981198489666, accuracy 0.11616922914981842\n",
            "Test: loss 2.3611492427928873, accuracy 0.10000014305114746\n",
            "Repeat - 0, Number of layers - 5, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.30809072548151, accuracy 0.14781838655471802\n",
            "Test: loss 2.2321169821397477, accuracy 0.196901336312294\n",
            "Repeat - 0, Number of layers - 5, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1831702361404894, accuracy 0.26996850967407227\n",
            "Test: loss 2.175786783491475, accuracy 0.2822079062461853\n",
            "Repeat - 0, Number of layers - 5, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.174899362683296, accuracy 0.2829363942146301\n",
            "Test: loss 2.1721164613649404, accuracy 0.28215789794921875\n",
            "Repeat - 0, Number of layers - 5, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.2264309713840484, accuracy 0.21828587353229523\n",
            "Test: loss 2.199339666466663, accuracy 0.2592695355415344\n",
            "Repeat - 0, Number of layers - 5, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.158653092056513, accuracy 0.29902008175849915\n",
            "Test: loss 2.0993452837084723, accuracy 0.3574199676513672\n",
            "Repeat - 0, Number of layers - 5, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1117193141281603, accuracy 0.34223780035972595\n",
            "Test: loss 2.102427612537744, accuracy 0.34362658858299255\n",
            "Repeat - 0, Number of layers - 5, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1660754416286947, accuracy 0.2877524793148041\n",
            "Test: loss 2.209273329917816, accuracy 0.24357779324054718\n",
            "Repeat - 0, Number of layers - 5, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1888275495171547, accuracy 0.2654193341732025\n",
            "Test: loss 2.188129670973839, accuracy 0.26141852140426636\n",
            "----------------------\n",
            "Repeat - 1, Number of layers - 5, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.89129714050889, accuracy 0.5618317723274231\n",
            "Test: loss 1.7876701160170685, accuracy 0.6731635332107544\n",
            "Repeat - 1, Number of layers - 5, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.807646693378687, accuracy 0.6532565355300903\n",
            "Test: loss 1.8404056482586726, accuracy 0.6206395030021667\n",
            "Repeat - 1, Number of layers - 5, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9840241672992707, accuracy 0.4770705997943878\n",
            "Test: loss 2.004028798936904, accuracy 0.45697084069252014\n",
            "Repeat - 1, Number of layers - 5, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.050903337806463, accuracy 0.4101404845714569\n",
            "Test: loss 2.096278122518731, accuracy 0.36481648683547974\n",
            "Repeat - 1, Number of layers - 5, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.09648874104023, accuracy 0.364605188369751\n",
            "Test: loss 2.182534824783119, accuracy 0.2786098122596741\n",
            "Repeat - 1, Number of layers - 5, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3389346680045127, accuracy 0.12220285087823868\n",
            "Test: loss 2.3611557658823177, accuracy 0.10000013560056686\n",
            "Repeat - 1, Number of layers - 5, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3611811980605126, accuracy 0.09996810555458069\n",
            "Test: loss 2.361100659020122, accuracy 0.10005004703998566\n",
            "Repeat - 1, Number of layers - 5, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3611505109667776, accuracy 0.10000159591436386\n",
            "Test: loss 2.3612006622097126, accuracy 0.09995010495185852\n",
            "Repeat - 1, Number of layers - 5, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.361149708300829, accuracy 0.10000160336494446\n",
            "Test: loss 2.3612006078774423, accuracy 0.09995013475418091\n",
            "Repeat - 1, Number of layers - 5, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.36115063560009, accuracy 0.10000158846378326\n",
            "Test: loss 2.3612006128817304, accuracy 0.0999501496553421\n",
            "----------------------\n",
            "Repeat - 2, Number of layers - 5, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8744594255387783, accuracy 0.5788829326629639\n",
            "Test: loss 1.7819465827727425, accuracy 0.6773110628128052\n",
            "Repeat - 2, Number of layers - 5, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8525872427225112, accuracy 0.6070356965065002\n",
            "Test: loss 1.8626762809067117, accuracy 0.5972514152526855\n",
            "Repeat - 2, Number of layers - 5, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.923469822973013, accuracy 0.5369820594787598\n",
            "Test: loss 1.8539590333355243, accuracy 0.6059970259666443\n",
            "Repeat - 2, Number of layers - 5, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8807390242516995, accuracy 0.5822994112968445\n",
            "Test: loss 1.8995183355983407, accuracy 0.5629187226295471\n",
            "Repeat - 2, Number of layers - 5, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8609757193922996, accuracy 0.6013820767402649\n",
            "Test: loss 1.9067476396260412, accuracy 0.5547228455543518\n",
            "Repeat - 2, Number of layers - 5, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.962374562472105, accuracy 0.498918741941452\n",
            "Test: loss 2.0523451461248667, accuracy 0.40919438004493713\n",
            "Repeat - 2, Number of layers - 5, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0388603420257567, accuracy 0.4222050607204437\n",
            "Test: loss 2.0382294717399794, accuracy 0.42293789982795715\n",
            "Repeat - 2, Number of layers - 5, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9596133088469505, accuracy 0.5013498663902283\n",
            "Test: loss 1.9798108593098585, accuracy 0.48125889897346497\n",
            "Repeat - 2, Number of layers - 5, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9555519691109657, accuracy 0.5057485103607178\n",
            "Test: loss 1.9059245522054418, accuracy 0.5556723475456238\n",
            "Repeat - 2, Number of layers - 5, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8938313381373881, accuracy 0.567130446434021\n",
            "Test: loss 1.898395778893352, accuracy 0.5617197155952454\n",
            "----------------------\n",
            "Repeat - 3, Number of layers - 5, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.093204512178898, accuracy 0.3636551797389984\n",
            "Test: loss 2.1779550935911094, accuracy 0.2828574478626251\n",
            "Repeat - 3, Number of layers - 5, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1691495072841644, accuracy 0.29202017188072205\n",
            "Test: loss 2.1106413558147836, accuracy 0.350223571062088\n",
            "Repeat - 3, Number of layers - 5, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.095794815629721, accuracy 0.36520546674728394\n",
            "Test: loss 2.0894829340424317, accuracy 0.371662974357605\n",
            "Repeat - 3, Number of layers - 5, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0863103905320166, accuracy 0.3746882975101471\n",
            "Test: loss 2.0651055309904747, accuracy 0.39565110206604004\n",
            "Repeat - 3, Number of layers - 5, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0555404476225374, accuracy 0.40553760528564453\n",
            "Test: loss 2.0029197781161034, accuracy 0.4584704637527466\n",
            "Repeat - 3, Number of layers - 5, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.031603490382433, accuracy 0.42957162857055664\n",
            "Test: loss 2.031036928497154, accuracy 0.430084228515625\n",
            "Repeat - 3, Number of layers - 5, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.023866157889366, accuracy 0.4372206926345825\n",
            "Test: loss 2.099615009828307, accuracy 0.3613678514957428\n",
            "Repeat - 3, Number of layers - 5, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0281558922827245, accuracy 0.4330219030380249\n",
            "Test: loss 2.010540089864602, accuracy 0.45057424902915955\n",
            "Repeat - 3, Number of layers - 5, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.018922764956951, accuracy 0.4421870708465576\n",
            "Test: loss 2.002382546529241, accuracy 0.4588199257850647\n",
            "Repeat - 3, Number of layers - 5, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0249917971491813, accuracy 0.43612128496170044\n",
            "Test: loss 2.024393988334793, accuracy 0.4367810785770416\n",
            "----------------------\n",
            "Repeat - 4, Number of layers - 5, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.2207300370037557, accuracy 0.2392868846654892\n",
            "Test: loss 2.2637433717990745, accuracy 0.19730132818222046\n",
            "Repeat - 4, Number of layers - 5, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.2876415818333626, accuracy 0.17345239222049713\n",
            "Test: loss 2.266586891178606, accuracy 0.19455264508724213\n",
            "Repeat - 4, Number of layers - 5, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.279211197912693, accuracy 0.18193650245666504\n",
            "Test: loss 2.268650593071327, accuracy 0.1925036460161209\n",
            "Repeat - 4, Number of layers - 5, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.2708845007419587, accuracy 0.1902695596218109\n",
            "Test: loss 2.264948191671357, accuracy 0.1962016224861145\n",
            "Repeat - 4, Number of layers - 5, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.2747792162299154, accuracy 0.18633627891540527\n",
            "Test: loss 2.3011317489029226, accuracy 0.16001978516578674\n",
            "Repeat - 4, Number of layers - 5, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.2957647724747656, accuracy 0.16540288925170898\n",
            "Test: loss 2.3062342567958574, accuracy 0.15487244725227356\n",
            "Repeat - 4, Number of layers - 5, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3603504313230514, accuracy 0.10080144554376602\n",
            "Test: loss 2.361150637023274, accuracy 0.10000007599592209\n",
            "Repeat - 4, Number of layers - 5, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.361150632917881, accuracy 0.10000152140855789\n",
            "Test: loss 2.361200606447646, accuracy 0.09995019435882568\n",
            "Repeat - 4, Number of layers - 5, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.361150633275509, accuracy 0.10000140219926834\n",
            "Test: loss 2.3612006085923407, accuracy 0.09995011240243912\n",
            "Repeat - 4, Number of layers - 5, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.361150635302067, accuracy 0.10000146925449371\n",
            "Test: loss 2.361100660807368, accuracy 0.10005006939172745\n",
            "----------------------\n",
            "Repeat - 0, Number of layers - 6, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0908606064319613, accuracy 0.3770008385181427\n",
            "Test: loss 1.9643239424503904, accuracy 0.5027986168861389\n",
            "Repeat - 0, Number of layers - 6, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9605055640935898, accuracy 0.502634584903717\n",
            "Test: loss 2.049120265683313, accuracy 0.4141921103000641\n",
            "Repeat - 0, Number of layers - 6, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0033851162195204, accuracy 0.45773768424987793\n",
            "Test: loss 2.0470355198658567, accuracy 0.4127427339553833\n",
            "Repeat - 0, Number of layers - 6, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0371472245156763, accuracy 0.42396992444992065\n",
            "Test: loss 2.0421357746424524, accuracy 0.41928932070732117\n",
            "Repeat - 0, Number of layers - 6, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0095359941124915, accuracy 0.4517865777015686\n",
            "Test: loss 2.0102153952153907, accuracy 0.4509739875793457\n",
            "Repeat - 0, Number of layers - 6, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0689971718788147, accuracy 0.3922393023967743\n",
            "Test: loss 2.0884166893394274, accuracy 0.37256237864494324\n",
            "Repeat - 0, Number of layers - 6, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.092349908798933, accuracy 0.36872220039367676\n",
            "Test: loss 2.0975480740931793, accuracy 0.36351677775382996\n",
            "Repeat - 0, Number of layers - 6, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.104183847486973, accuracy 0.35683879256248474\n",
            "Test: loss 2.1613893868147525, accuracy 0.29984891414642334\n",
            "Repeat - 0, Number of layers - 6, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.1703569779992105, accuracy 0.29063668847084045\n",
            "Test: loss 2.126190167138244, accuracy 0.33573073148727417\n",
            "Repeat - 0, Number of layers - 6, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.097576270341873, accuracy 0.36403870582580566\n",
            "Test: loss 2.0878828483364216, accuracy 0.3738117516040802\n",
            "----------------------\n",
            "Repeat - 1, Number of layers - 6, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9457658122777939, accuracy 0.5188620090484619\n",
            "Test: loss 1.8409243625500749, accuracy 0.6192403435707092\n",
            "Repeat - 1, Number of layers - 6, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8289857198894024, accuracy 0.6271341443061829\n",
            "Test: loss 1.8084817551065242, accuracy 0.6497756242752075\n",
            "Repeat - 1, Number of layers - 6, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8321422680020332, accuracy 0.6241039633750916\n",
            "Test: loss 1.819429307684548, accuracy 0.6222889423370361\n",
            "Repeat - 1, Number of layers - 6, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8406500658094882, accuracy 0.6152368187904358\n",
            "Test: loss 1.8357600760066706, accuracy 0.6076461672782898\n",
            "Repeat - 1, Number of layers - 6, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8637091539502144, accuracy 0.5872136950492859\n",
            "Test: loss 1.9167656955690398, accuracy 0.5260371565818787\n",
            "Repeat - 1, Number of layers - 6, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9494414962232114, accuracy 0.49491629004478455\n",
            "Test: loss 1.9017431666289848, accuracy 0.550225019454956\n",
            "Repeat - 1, Number of layers - 6, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9042629864513874, accuracy 0.5485652089118958\n",
            "Test: loss 1.9011665621976266, accuracy 0.5569213628768921\n",
            "Repeat - 1, Number of layers - 6, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.923833717405796, accuracy 0.5314467549324036\n",
            "Test: loss 1.8909555659301278, accuracy 0.5610194802284241\n",
            "Repeat - 1, Number of layers - 6, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.882782532721758, accuracy 0.5685986876487732\n",
            "Test: loss 1.8819265867816632, accuracy 0.5734632015228271\n",
            "Repeat - 1, Number of layers - 6, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8866748496592045, accuracy 0.5702819228172302\n",
            "Test: loss 1.88467343958064, accuracy 0.5728634595870972\n",
            "----------------------\n",
            "Repeat - 2, Number of layers - 6, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3276514676213265, accuracy 0.12440317124128342\n",
            "Test: loss 2.36105068334039, accuracy 0.100100077688694\n",
            "Repeat - 2, Number of layers - 6, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3611506354808807, accuracy 0.10000146925449371\n",
            "Test: loss 2.3610506844127377, accuracy 0.10010004788637161\n",
            "Repeat - 2, Number of layers - 6, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3611506363749504, accuracy 0.10000161081552505\n",
            "Test: loss 2.361150628980668, accuracy 0.10000014305114746\n",
            "Repeat - 2, Number of layers - 6, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3611506347060205, accuracy 0.10000152885913849\n",
            "Test: loss 2.3612006100221374, accuracy 0.0999501571059227\n",
            "Repeat - 2, Number of layers - 6, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3611506343483923, accuracy 0.10000152140855789\n",
            "Test: loss 2.3612006103795866, accuracy 0.0999501571059227\n",
            "Repeat - 2, Number of layers - 6, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.361150633811951, accuracy 0.1000015065073967\n",
            "Test: loss 2.3611506336275068, accuracy 0.10000018030405045\n",
            "Repeat - 2, Number of layers - 6, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3611506341695785, accuracy 0.10000158846378326\n",
            "Test: loss 2.3612006128817304, accuracy 0.0999501496553421\n",
            "Repeat - 2, Number of layers - 6, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.361150634139776, accuracy 0.10000132769346237\n",
            "Test: loss 2.3611506350573035, accuracy 0.10000013560056686\n",
            "Repeat - 2, Number of layers - 6, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3611506327390672, accuracy 0.10000137984752655\n",
            "Test: loss 2.3611506318402613, accuracy 0.10000013560056686\n",
            "Repeat - 2, Number of layers - 6, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.3611506351828577, accuracy 0.10000159591436386\n",
            "Test: loss 2.36120060894979, accuracy 0.09995013475418091\n",
            "----------------------\n",
            "Repeat - 3, Number of layers - 6, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9799442319273948, accuracy 0.4788975417613983\n",
            "Test: loss 1.9023667793760057, accuracy 0.5554224252700806\n",
            "Repeat - 3, Number of layers - 6, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9227175305485726, accuracy 0.5315487384796143\n",
            "Test: loss 1.8997370354954093, accuracy 0.5518742799758911\n",
            "Repeat - 3, Number of layers - 6, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.81672328799963, accuracy 0.642022967338562\n",
            "Test: loss 1.7768662068797374, accuracy 0.6828084588050842\n",
            "Repeat - 3, Number of layers - 6, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7650983091294765, accuracy 0.693777859210968\n",
            "Test: loss 1.8267019704721499, accuracy 0.6358819603919983\n",
            "Repeat - 3, Number of layers - 6, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.855805292993784, accuracy 0.6045519113540649\n",
            "Test: loss 1.82427485307296, accuracy 0.6365818977355957\n",
            "Repeat - 3, Number of layers - 6, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8100722546875476, accuracy 0.6505402326583862\n",
            "Test: loss 1.7807541140194596, accuracy 0.6803094744682312\n",
            "Repeat - 3, Number of layers - 6, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8666057171821595, accuracy 0.5946075320243835\n",
            "Test: loss 1.9969067993550107, accuracy 0.4645175337791443\n",
            "Repeat - 3, Number of layers - 6, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9549725510776044, accuracy 0.5053181648254395\n",
            "Test: loss 1.998692955212972, accuracy 0.4607694149017334\n",
            "Repeat - 3, Number of layers - 6, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9941588406562805, accuracy 0.4661523401737213\n",
            "Test: loss 1.9916839828376827, accuracy 0.4688149690628052\n",
            "Repeat - 3, Number of layers - 6, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.014002679079771, accuracy 0.4451875388622284\n",
            "Test: loss 2.081864034575501, accuracy 0.37711021304130554\n",
            "----------------------\n",
            "Repeat - 4, Number of layers - 6, Epoch - 0\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9662200724780559, accuracy 0.4864797592163086\n",
            "Test: loss 1.8815269640003187, accuracy 0.5814592242240906\n",
            "Repeat - 4, Number of layers - 6, Epoch - 1\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.7983126206994056, accuracy 0.6633404493331909\n",
            "Test: loss 1.7980416871975924, accuracy 0.6630682945251465\n",
            "Repeat - 4, Number of layers - 6, Epoch - 2\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8648189647495748, accuracy 0.5961501002311707\n",
            "Test: loss 1.87167792520423, accuracy 0.5896556377410889\n",
            "Repeat - 4, Number of layers - 6, Epoch - 3\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9147457934319974, accuracy 0.5461978912353516\n",
            "Test: loss 1.9681001167068597, accuracy 0.49195346236228943\n",
            "Repeat - 4, Number of layers - 6, Epoch - 4\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9391479712724686, accuracy 0.5217813849449158\n",
            "Test: loss 1.870741758568176, accuracy 0.590155303478241\n",
            "Repeat - 4, Number of layers - 6, Epoch - 5\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.8578145858347417, accuracy 0.6032341122627258\n",
            "Test: loss 2.0724403043200765, accuracy 0.3886546194553375\n",
            "Repeat - 4, Number of layers - 6, Epoch - 6\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9226906107068062, accuracy 0.538349449634552\n",
            "Test: loss 1.9469750765738996, accuracy 0.5141433477401733\n",
            "Repeat - 4, Number of layers - 6, Epoch - 7\n",
            "Current learning rate: 0.001\n",
            "Train: loss 1.9863783604204654, accuracy 0.47470390796661377\n",
            "Test: loss 2.0351750384206357, accuracy 0.42588603496551514\n",
            "Repeat - 4, Number of layers - 6, Epoch - 8\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.0609356778860093, accuracy 0.4001704454421997\n",
            "Test: loss 2.0186768782728617, accuracy 0.44237878918647766\n",
            "Repeat - 4, Number of layers - 6, Epoch - 9\n",
            "Current learning rate: 0.001\n",
            "Train: loss 2.022192810446024, accuracy 0.438921719789505\n",
            "Test: loss 2.0132966373992645, accuracy 0.4478253126144409\n",
            "----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_kL1T-keK-p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "d6fd40a0-f43d-45ef-c66f-196641832136"
      },
      "source": [
        "plt.plot(layers, losses_train, 'ro-', label='Loss on train', color='blue')\n",
        "plt.plot(layers, losses_test, 'ro-', label='Loss on test', color='green')\n",
        "plt.xlabel(\"Number of layers\")\n",
        "plt.title(\"Loss on train and test\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0De48IXWoQgZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f72ce9c3-0078-45e1-9052-4b1a005e1785"
      },
      "source": [
        "plt.plot(layers, accs_train, 'ro-', label='Accuracy on train', color='blue')\n",
        "plt.plot(layers, accs_test, 'ro-', label='Accuracy on test', color='green')\n",
        "plt.xlabel(\"Number of layers\")\n",
        "plt.title(\"Accuracy on train and test\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRLj815jd52Z"
      },
      "source": [
        "**Выводы**\n",
        "\n",
        "1. Ошибка на тесте и трейне практически одинаковы, что говорит о том, что модель слишком простая.\n",
        "\n",
        "2. Лучшее качество было получено при 2-х слоях.\n",
        "\n",
        "3. При числе слоев больше 2-х, модель обучается хуже. Ведь размерность скрытых слоев одинакова и, кажется, что начинаются проблемы с градиентами при обратном распространении, и функция хуже минимизируется. \n",
        "Но здесь стоит сделать ремарку о том, что начальная инициализация весов очень сильно влияет на конечный результат. Возможно, поигравшись с ней, можно было достигнуть и лучших результатов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3vcONKQ86mu"
      },
      "source": [
        "## 4. Бонусная часть."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuhxy7aW86mu"
      },
      "source": [
        "### 4.1 Реализация метода оптимизации (3 + 3 балла).\n",
        "Реализуйте сами метод оптимизации  для рассмотренной выше архитектуры. Вы можете выбрать произвольный метод от классики до супермодных вариантов. Продемонстрируйте правильную работу метода оптимизации, сравните его работы с Adam. \n",
        "\n",
        "**Дополнительные баллы** вы получите, если метод будет уникален среди сдавших задание. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_CYcJgVxbeb"
      },
      "source": [
        "Ниже приведен пример реализации SGD в рамках фреймворка **PyTorch.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYMu0uF1p6o0"
      },
      "source": [
        "class MySotaOptimizer(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3):\n",
        "        defaults = dict(lr=lr)\n",
        "        super(MySotaOptimizer, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self,):\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    p.data.add_(-lr*p.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3aabYCykXL7"
      },
      "source": [
        "### 4.2 Реализация модной активации (2 + 2 балла).\n",
        "Реализуйте одну из активаций, предложенных на лекции или в статье. Например, Hardswish. Сравните сеть с вашей активацией и с ReLU. \n",
        "\n",
        "**Дополнительные баллы** вы получите, если функция будет уникальна среди сдавших задание. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_rEcuA-StZv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
