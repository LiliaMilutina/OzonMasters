{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OM3_2021_HW3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUS9nDP1D1qh"
      },
      "source": [
        "# **Домашнее задание #3. Шагающие человечки**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPJakN_ZsLx6"
      },
      "source": [
        "## **Часть 0. Вспоминаем код архитектуры и проверяем работоспособность детектора**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsjaU-JQsT4I"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import nn\n",
        "from math import sqrt\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEbKgaDsg9ts"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zyEILKasX1N"
      },
      "source": [
        "## VGG Backbone\n",
        "class VGGBase(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGGBase, self).__init__()\n",
        "\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
        "\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)  \n",
        "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "\n",
        "        self.load_pretrained_layers()\n",
        "\n",
        "    def forward(self, image):\n",
        "        out = F.relu(self.conv1_1(image))\n",
        "        out = F.relu(self.conv1_2(out))\n",
        "        out = self.pool1(out)\n",
        "\n",
        "        out = F.relu(self.conv2_1(out))\n",
        "        out = F.relu(self.conv2_2(out))\n",
        "        out = self.pool2(out)\n",
        "\n",
        "        out = F.relu(self.conv3_1(out))\n",
        "        out = F.relu(self.conv3_2(out))\n",
        "        out = F.relu(self.conv3_3(out)) \n",
        "        out = self.pool3(out)\n",
        "\n",
        "        out = F.relu(self.conv4_1(out))\n",
        "        out = F.relu(self.conv4_2(out))\n",
        "        out = F.relu(self.conv4_3(out))\n",
        "        conv4_3_feats = out\n",
        "\n",
        "        out = self.pool4(out)\n",
        "        out = F.relu(self.conv5_1(out))\n",
        "        out = F.relu(self.conv5_2(out))\n",
        "        out = F.relu(self.conv5_3(out))\n",
        "        out = self.pool5(out)           \n",
        "        out = F.relu(self.conv6(out))   \n",
        "        conv7_feats = F.relu(self.conv7(out))\n",
        "\n",
        "        return conv4_3_feats, conv7_feats\n",
        "\n",
        "    def load_pretrained_layers(self):\n",
        "        state_dict = self.state_dict()\n",
        "        param_names = list(state_dict.keys())\n",
        "\n",
        "        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n",
        "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
        "\n",
        "        for i, param in enumerate(param_names[:-4]): \n",
        "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
        "\n",
        "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)\n",
        "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']\n",
        "        \n",
        "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3]) \n",
        "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])\n",
        "        \n",
        "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)\n",
        "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias'] \n",
        "\n",
        "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None]) \n",
        "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])\n",
        "\n",
        "        self.load_state_dict(state_dict)\n",
        "\n",
        "        print(\"\\n Loaded backbone.\\n\")\n",
        "\n",
        "def decimate(tensor, m):\n",
        "    assert tensor.dim() == len(m)\n",
        "    for d in range(tensor.dim()):\n",
        "        if m[d] is not None:\n",
        "            tensor = tensor.index_select(dim=d, index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n",
        "    return tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxiDRJX5tS6R"
      },
      "source": [
        "class AuxiliaryConvolutions(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AuxiliaryConvolutions, self).__init__()\n",
        "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0) \n",
        "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)\n",
        "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
        "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)\n",
        "\n",
        "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
        "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)\n",
        "\n",
        "    def forward(self, conv7_feats):\n",
        "        out = F.relu(self.conv8_1(conv7_feats)) \n",
        "        out = F.relu(self.conv8_2(out))  \n",
        "        conv8_2_feats = out \n",
        "\n",
        "        out = F.relu(self.conv9_1(out))\n",
        "        out = F.relu(self.conv9_2(out)) \n",
        "        conv9_2_feats = out \n",
        "\n",
        "        out = F.relu(self.conv10_1(out)) \n",
        "        out = F.relu(self.conv10_2(out))\n",
        "        conv10_2_feats = out  \n",
        "\n",
        "        out = F.relu(self.conv11_1(out))\n",
        "        out = F.relu(self.conv11_2(out))\n",
        "        conv11_2_feats = out\n",
        "\n",
        "        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats\n",
        "\n",
        "\n",
        "class PredictionConvolutions(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(PredictionConvolutions, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        n_boxes = {'conv4_3': 4, 'conv7': 6, 'conv8_2': 6,\n",
        "                   'conv9_2': 6, 'conv10_2': 4, 'conv11_2': 4}\n",
        "\n",
        "        self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * 4, kernel_size=3, padding=1)\n",
        "\n",
        "        self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):\n",
        "        batch_size = conv4_3_feats.size(0)\n",
        "\n",
        "        l_conv4_3 = self.loc_conv4_3(conv4_3_feats) \n",
        "        l_conv4_3 = l_conv4_3.permute(0, 2, 3, 1).contiguous()\n",
        "        l_conv4_3 = l_conv4_3.view(batch_size, -1, 4)\n",
        "\n",
        "        l_conv7 = self.loc_conv7(conv7_feats)\n",
        "        l_conv7 = l_conv7.permute(0, 2, 3, 1).contiguous()\n",
        "        l_conv7 = l_conv7.view(batch_size, -1, 4)\n",
        "\n",
        "        l_conv8_2 = self.loc_conv8_2(conv8_2_feats)\n",
        "        l_conv8_2 = l_conv8_2.permute(0, 2, 3, 1).contiguous()\n",
        "        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4)\n",
        "\n",
        "        l_conv9_2 = self.loc_conv9_2(conv9_2_feats)\n",
        "        l_conv9_2 = l_conv9_2.permute(0, 2, 3, 1).contiguous()\n",
        "        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4)\n",
        "\n",
        "        l_conv10_2 = self.loc_conv10_2(conv10_2_feats) \n",
        "        l_conv10_2 = l_conv10_2.permute(0, 2, 3, 1).contiguous()\n",
        "        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4)\n",
        "\n",
        "        l_conv11_2 = self.loc_conv11_2(conv11_2_feats) \n",
        "        l_conv11_2 = l_conv11_2.permute(0, 2, 3, 1).contiguous()\n",
        "        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4)\n",
        "\n",
        "        c_conv4_3 = self.cl_conv4_3(conv4_3_feats)\n",
        "        c_conv4_3 = c_conv4_3.permute(0, 2, 3, 1).contiguous()\n",
        "        c_conv4_3 = c_conv4_3.view(batch_size, -1, self.n_classes)\n",
        "\n",
        "        c_conv7 = self.cl_conv7(conv7_feats)\n",
        "        c_conv7 = c_conv7.permute(0, 2, 3, 1).contiguous()\n",
        "        c_conv7 = c_conv7.view(batch_size, -1,self.n_classes)\n",
        "\n",
        "        c_conv8_2 = self.cl_conv8_2(conv8_2_feats)\n",
        "        c_conv8_2 = c_conv8_2.permute(0, 2, 3, 1).contiguous()\n",
        "        c_conv8_2 = c_conv8_2.view(batch_size, -1, self.n_classes)\n",
        "\n",
        "        c_conv9_2 = self.cl_conv9_2(conv9_2_feats)\n",
        "        c_conv9_2 = c_conv9_2.permute(0, 2, 3, 1).contiguous()\n",
        "        c_conv9_2 = c_conv9_2.view(batch_size, -1, self.n_classes)\n",
        "\n",
        "        c_conv10_2 = self.cl_conv10_2(conv10_2_feats)\n",
        "        c_conv10_2 = c_conv10_2.permute(0, 2, 3, 1).contiguous()\n",
        "        c_conv10_2 = c_conv10_2.view(batch_size, -1, self.n_classes)\n",
        "\n",
        "        c_conv11_2 = self.cl_conv11_2(conv11_2_feats)\n",
        "        c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()\n",
        "        c_conv11_2 = c_conv11_2.view(batch_size, -1, self.n_classes)\n",
        "\n",
        "        locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)\n",
        "        classes_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2], dim=1)\n",
        "\n",
        "        return locs, classes_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlz2yZNUtuse"
      },
      "source": [
        "class SSD300_(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(SSD300_, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.base = VGGBase()\n",
        "        self.aux_convs = AuxiliaryConvolutions()\n",
        "        self.pred_convs = PredictionConvolutions(n_classes)\n",
        "\n",
        "        self.priors_cxcy = self.create_prior_boxes()\n",
        "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1)) \n",
        "        nn.init.constant_(self.rescale_factors, 20)\n",
        "\n",
        "    def create_prior_boxes(self):\n",
        "        fmap_dims = {'conv4_3': 38, 'conv7': 19, 'conv8_2': 10,\n",
        "                     'conv9_2': 5, 'conv10_2': 3, 'conv11_2': 1}\n",
        "\n",
        "        obj_scales = {'conv4_3': 0.1, 'conv7': 0.2, 'conv8_2': 0.375,\n",
        "                      'conv9_2': 0.55, 'conv10_2': 0.725, 'conv11_2': 0.9}\n",
        "        \n",
        "        aspect_ratios = {'conv4_3': [1., 2., 0.5], 'conv7': [1., 2., 3., 0.5, 0.333],\n",
        "                         'conv8_2': [1., 2., 3., 0.5, 0.333], 'conv9_2': [1., 2., 3., 0.5, 0.333],\n",
        "                         'conv10_2': [1., 2., 0.5], 'conv11_2': [1., 2., 0.5]}\n",
        "\n",
        "        prior_boxes = []\n",
        "        fmaps = list(fmap_dims.keys())\n",
        "\n",
        "        for k, fmap in enumerate(fmaps):\n",
        "            for i in range(fmap_dims[fmap]): \n",
        "                for j in range(fmap_dims[fmap]): \n",
        "                    cx = (j + 0.5) / fmap_dims[fmap]\n",
        "                    cy = (i + 0.5) / fmap_dims[fmap]\n",
        "\n",
        "                    for ratio in aspect_ratios[fmap]:\n",
        "                        prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])\n",
        "\n",
        "                        if ratio == 1.:\n",
        "                            try:\n",
        "                                additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\n",
        "\n",
        "                            except IndexError:\n",
        "                                additional_scale = 1.\n",
        "                            prior_boxes.append([cx, cy, additional_scale, additional_scale])\n",
        "\n",
        "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)\n",
        "        prior_boxes.clamp_(0, 1)\n",
        "        return prior_boxes\n",
        "\n",
        "    def forward(self, image):\n",
        "      conv4_3_feats, conv7_feats = self.base(image)  \n",
        "\n",
        "      norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt()\n",
        "      conv4_3_feats = conv4_3_feats / norm \n",
        "      conv4_3_feats = conv4_3_feats * self.rescale_factors  \n",
        "\n",
        "      conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = self.aux_convs(conv7_feats)  \n",
        "\n",
        "      locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, \n",
        "                                              conv10_2_feats, conv11_2_feats)\n",
        "\n",
        "      return locs, classes_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc0tkBHMzX8U"
      },
      "source": [
        "#Вспомогательные функции\n",
        "#(c_x, c_y, w, h) -> (x_min, y_min, x_max, y_max)\n",
        "def cxcy_to_xy(cxcy):\n",
        "    return torch.cat([cxcy[:, :2] - (cxcy[:, 2:] / 2),  # x_min, y_min\n",
        "                      cxcy[:, :2] + (cxcy[:, 2:] / 2)], 1)  # x_max, y_max\n",
        "\n",
        "#(c_x, c_y, w, h) -> (g_c_x, g_c_y,  g_w, g_h)\n",
        "def cxcy_to_gcxgcy(cxcy, priors_cxcy):\n",
        "    return torch.cat([(cxcy[:, :2] - priors_cxcy[:, :2]) / (priors_cxcy[:, 2:] / 10), \n",
        "                      torch.log(cxcy[:, 2:] / priors_cxcy[:, 2:]) * 5], 1) \n",
        "\n",
        "#(g_c_x, g_c_y,  g_w, g_h) -> (c_x, c_y, w, h)\n",
        "def gcxgcy_to_cxcy(gcxgcy, priors_cxcy):\n",
        "    return torch.cat([gcxgcy[:, :2] * priors_cxcy[:, 2:] / 10 + priors_cxcy[:, :2], \n",
        "                      torch.exp(gcxgcy[:, 2:] / 5) * priors_cxcy[:, 2:]], 1)\n",
        "    \n",
        "\n",
        "def xy_to_cxcy(xy):\n",
        "    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  # c_x, c_y\n",
        "                      xy[:, 2:] - xy[:, :2]], 1)  # w, h\n",
        "\n",
        "\n",
        "def find_intersection(set_1, set_2):\n",
        "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  \n",
        "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0)) \n",
        "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0) \n",
        "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1] \n",
        "\n",
        "\n",
        "def find_jaccard_overlap(set_1, set_2):\n",
        "    intersection = find_intersection(set_1, set_2) \n",
        "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])\n",
        "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])\n",
        "\n",
        "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection \n",
        "    return intersection / union"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJtgCBe2wEkl"
      },
      "source": [
        "class SSD300(SSD300_):\n",
        "  def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):\n",
        "        batch_size = predicted_locs.size(0)\n",
        "        n_priors = self.priors_cxcy.size(0)\n",
        "\n",
        "        predicted_scores = F.softmax(predicted_scores, dim=2)\n",
        "\n",
        "        all_images_boxes = list()\n",
        "        all_images_labels = list()\n",
        "        all_images_scores = list()\n",
        "\n",
        "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            decoded_locs = cxcy_to_xy(gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))\n",
        "\n",
        "            image_boxes = list()\n",
        "            image_labels = list()\n",
        "            image_scores = list()\n",
        "\n",
        "            max_scores, best_label = predicted_scores[i].max(dim=1) \n",
        "\n",
        "            for c in range(1, self.n_classes):\n",
        "                class_scores = predicted_scores[i][:, c]\n",
        "                score_above_min_score = class_scores > min_score \n",
        "                n_above_min_score = score_above_min_score.sum().item()\n",
        "                if n_above_min_score == 0:\n",
        "                    continue\n",
        " \n",
        "                class_scores = class_scores[score_above_min_score]\n",
        "                class_decoded_locs = decoded_locs[score_above_min_score] \n",
        "\n",
        "                class_scores, sort_ind = class_scores.sort(dim=0, descending=True)  \n",
        "                class_decoded_locs = class_decoded_locs[sort_ind] \n",
        "\n",
        "                overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)  \n",
        "\n",
        "                # Non-Maximum Suppression (NMS)\n",
        "                suppress = torch.zeros((n_above_min_score), dtype=torch.uint8).to(device)\n",
        "\n",
        "                for box in range(class_decoded_locs.size(0)):\n",
        "                    if suppress[box] == 1:\n",
        "                        continue\n",
        "\n",
        "                    suppress = suppress | (overlap[box] > max_overlap)\n",
        "                    suppress[box] = 0\n",
        "\n",
        "                image_boxes.append(class_decoded_locs[1 - suppress])\n",
        "                image_labels.append(torch.LongTensor((1 - suppress).sum().item() * [c]).to(device))\n",
        "                image_scores.append(class_scores[1 - suppress])\n",
        "\n",
        "            if len(image_boxes) == 0:\n",
        "                image_boxes.append(torch.FloatTensor([[0., 0., 1., 1.]]).to(device))\n",
        "                image_labels.append(torch.LongTensor([0]).to(device))\n",
        "                image_scores.append(torch.FloatTensor([0.]).to(device))\n",
        "\n",
        "            image_boxes = torch.cat(image_boxes, dim=0) \n",
        "            image_labels = torch.cat(image_labels, dim=0)\n",
        "            image_scores = torch.cat(image_scores, dim=0) \n",
        "            n_objects = image_scores.size(0)\n",
        "\n",
        "            if n_objects > top_k:\n",
        "                image_scores, sort_ind = image_scores.sort(dim=0, descending=True)\n",
        "                image_scores = image_scores[:top_k] \n",
        "                image_boxes = image_boxes[sort_ind][:top_k] \n",
        "                image_labels = image_labels[sort_ind][:top_k] \n",
        "\n",
        "            all_images_boxes.append(image_boxes)\n",
        "            all_images_labels.append(image_labels)\n",
        "            all_images_scores.append(image_scores)\n",
        "\n",
        "        return all_images_boxes, all_images_labels, all_images_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVFwO1rdX_yl"
      },
      "source": [
        "class MultiBoxLoss(nn.Module):\n",
        "    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.):\n",
        "        super(MultiBoxLoss, self).__init__()\n",
        "\n",
        "        self.priors_cxcy = priors_cxcy\n",
        "        self.priors_xy = cxcy_to_xy(priors_cxcy)\n",
        "        self.threshold = threshold \n",
        "        self.neg_pos_ratio = neg_pos_ratio \n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.smooth_l1 = nn.L1Loss()\n",
        "        self.cross_entropy = nn.CrossEntropyLoss(reduce=False)\n",
        "\n",
        "    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n",
        "        batch_size = predicted_locs.size(0)\n",
        "        n_priors = self.priors_cxcy.size(0)\n",
        "        n_classes = predicted_scores.size(2)\n",
        "\n",
        "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
        "\n",
        "        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device)\n",
        "        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device)\n",
        "\n",
        "        #каждую картинку отдельно обрабатываем\n",
        "        for i in range(batch_size):\n",
        "            n_objects = boxes[i].size(0)\n",
        "            overlap = find_jaccard_overlap(boxes[i].to(device), self.priors_xy) \n",
        "            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)\n",
        "            _, prior_for_each_object = overlap.max(dim=1)\n",
        "            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n",
        "            overlap_for_each_prior[prior_for_each_object] = 1.\n",
        "            label_for_each_prior = labels[i][object_for_each_prior]\n",
        "            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0\n",
        "            true_classes[i] = label_for_each_prior\n",
        "            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]).to(device), self.priors_cxcy)  # (8732, 4)\n",
        "\n",
        "        positive_priors = true_classes != 0\n",
        "\n",
        "        # LOCALIZATION LOSS\n",
        "        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])  #scalar\n",
        "\n",
        "        # CONFIDENCE LOSS\n",
        "        n_positives = positive_priors.sum(dim=1)\n",
        "        n_hard_negatives = self.neg_pos_ratio * n_positives\n",
        "\n",
        "        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))\n",
        "        conf_loss_all = conf_loss_all.view(batch_size, n_priors) \n",
        "        conf_loss_pos = conf_loss_all[positive_priors]\n",
        "        conf_loss_neg = conf_loss_all.clone()\n",
        "        conf_loss_neg[positive_priors] = 0.  \n",
        "        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)\n",
        "\n",
        "        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)\n",
        "        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)\n",
        "        conf_loss_hard_neg = conf_loss_neg[hard_negatives]\n",
        "\n",
        "        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()\n",
        "\n",
        "        return conf_loss + self.alpha * loc_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Kv0-SKeyI5m"
      },
      "source": [
        "#будем использовать модель предобученную на Pascal VOC, вот его классы\n",
        "voc_labels = ('background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
        "              'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
        "\n",
        "label_map = {k: v for v, k in enumerate(voc_labels)}\n",
        "rev_label_map = {v: k for k, v in label_map.items()} \n",
        "\n",
        "# Color map for bounding boxes of detected objects from https://sashat.me/2017/01/11/list-of-20-simple-distinct-colors/\n",
        "distinct_colors = ['#e6194b', '#3cb44b', '#ffe119', '#0082c8', '#f58231', '#911eb4', '#46f0f0', '#f032e6',\n",
        "                   '#d2f53c', '#fabebe', '#008080', '#000080', '#aa6e28', '#fffac8', '#800000', '#aaffc3', '#808000',\n",
        "                   '#ffd8b1', '#e6beff', '#808080', '#FFFFFF']\n",
        "label_color_map = {k: distinct_colors[i] for i, k in enumerate(label_map.keys())}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2Dj-sxUWlUT"
      },
      "source": [
        "### **Задание 0** (1 балл) \n",
        "\n",
        "[Скачайте](https://drive.google.com/file/d/1ySBa0zbcd-59zan-8c0gwbmywd8dEq5h/view?usp=sharing) обученные на `Pascal VOC` веса для этой модели. Положите их себе на drive.\n",
        "\n",
        "Подгрузите веса и протестите ssd на [одной из картинок](https://www.google.com/url?sa=i&url=https%3A%2F%2Fvk.com%2F%40shizpub-ochen-dovolnyi-pesel&psig=AOvVaw0ef1A0Qgda7GjhW3d9Fs-w&ust=1635250162815000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCMiZmt_D5fMCFQAAAAAdAAAAABAE) (мы же хотим убедиться что с весами всё ок)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RAVfjj0xQJU"
      },
      "source": [
        "ssd = SSD300(len(voc_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFmrBTtQQSxQ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bePQpmQVQV4r"
      },
      "source": [
        "data_path = 'gdrive/MyDrive'\n",
        "checkpoint = torch.load(data_path + '/checkpoint_ssd300_pascal_voc.pth', map_location=device)\n",
        "ssd.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq78L8IkC-5e"
      },
      "source": [
        "ssd = ssd.to(device)\n",
        "ssd.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wvK2SyERPgU"
      },
      "source": [
        "resize = transforms.Resize((300, 300))\n",
        "to_tensor = transforms.ToTensor()\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61jFU-zzRatS"
      },
      "source": [
        "path_to_img = data_path + '/1.jpg'\n",
        "original_image = Image.open(path_to_img)\n",
        "\n",
        "transform = transforms.Compose([resize, to_tensor, normalize])\n",
        "img = transform(original_image).to(device)\n",
        "img = img[None, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPPYEWncRmEp"
      },
      "source": [
        "locs, scores = ssd(img)\n",
        "det_boxes, det_labels, det_scores = ssd.detect_objects(locs.detach(), scores.detach(), min_score=0.7, max_overlap=0.5, top_k=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cGwnfLvR9mz"
      },
      "source": [
        "original_dims = torch.FloatTensor([original_image.width, original_image.height, original_image.width, original_image.height]).unsqueeze(0).to(device)\n",
        "det_boxes = det_boxes[0] * original_dims\n",
        "det_labels = [rev_label_map[l] for l in det_labels[0].to('cpu').tolist()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eapY688nTCiZ"
      },
      "source": [
        "def draw_boxes(original_image, det_boxes, det_labels, label_color_map):\n",
        "  annotated_image = original_image.copy()\n",
        "  draw = ImageDraw.Draw(annotated_image)\n",
        "  font = ImageFont.load_default()\n",
        "\n",
        "  for i in range(det_boxes.size(0)):\n",
        "    # Boxes\n",
        "    box_location = det_boxes[i].tolist()\n",
        "    draw.rectangle(xy=box_location, outline=label_color_map[det_labels[i]])\n",
        "    draw.rectangle(xy=[l + 1. for l in box_location], outline=label_color_map[det_labels[i]]) \n",
        "\n",
        "    # Text\n",
        "    text_size = font.getsize(det_labels[i].upper())\n",
        "    text_location = [box_location[0] + 2., box_location[1] - text_size[1]]\n",
        "    textbox_location = [box_location[0], box_location[1] - text_size[1], box_location[0] + text_size[0] + 4., box_location[1]]\n",
        "    draw.rectangle(xy=textbox_location, fill=label_color_map[det_labels[i]])\n",
        "    draw.text(xy=text_location, text=det_labels[i].upper(), fill='white',font=font)\n",
        "  return annotated_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd2VvQZVT-Fv"
      },
      "source": [
        "draw_boxes(original_image, det_boxes, det_labels, label_color_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT9FWjmQZ_YH"
      },
      "source": [
        "Попробуйте поиграться с гиперпараметрами чтобы получилось лучшее качаство на этой картинке. \n",
        "\n",
        "В целом результат не слишком великолепен - обучен детектор на небольшой базе `Pascal VOC`, мы используем не самую сильную его версию - `ssd300`, но даже с такими вводными у нас получится сделать интересное, let's get it started. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvEEmNUjEHTV"
      },
      "source": [
        "## **Часть 1. Копаемся в данных**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOGh1li7sLy6"
      },
      "source": [
        "На этот раз  у нас будет небольшой датасет из гуляющих людей (вы удивитесь насколько небольшой). Данные можно скачать [здесь](https://drive.google.com/file/d/1_8mnenRzV6ylr7L29LuhQHKd82GNzU9y/view?usp=sharing).\n",
        "\n",
        "Для того, чтобы учить сеть, нужен будет `Dataloader`, а значит и собственный `Dataset`. Чтобы собрать `Dataset`, надо понять как выглядят данные и аннотация, а также какие данные понадобятся вашей сетке для обучения. Данные будут не совсем типичные для задачи - у вас будут картинки и маски объектов, из масок вам самим придется сделать боксы, которые так жаждет наш `MultiBoxLoss`.\n",
        "\n",
        "Данные уже разделены на `train` и `test`, никакого кровосмешения больше не делаем.\n",
        "\n",
        "**Нужно, чтобы данные лежали у вас архивом на Гуглдиске, тогда, поменяв директорию, ваши результаты я смогу воспроизвести и оценить.**\n",
        "\n",
        "**NB**. Часть с копанием в данных приносит вам баллы, только если далее вы попробуете что-то осмысленное сделать с сеткой. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K03u6y9o4fqE"
      },
      "source": [
        "### **Задание 1** (1 балл)\n",
        "Разархивируйте данные, загрузите и посмотрите на фото и соответствующие им маски (соответствие происходит явно по имени), напишите функцию, которая по маске высчитывает бокс, отрисуйте 4-5 фото с полученными боксами. Эту функцию будем использовать в `Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuSRznqp4glf"
      },
      "source": [
        "!unzip -q '/content/gdrive/MyDrive/PennFudanPed.zip' -d './'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrTvtUfBOj3q"
      },
      "source": [
        "# получение названий изображений и соответствующих им масок\n",
        "\n",
        "def get_imgs_and_masks(root, is_train):\n",
        "    imgs = []\n",
        "    masks = []\n",
        "\n",
        "    if is_train:\n",
        "      image_folder_path = root + 'Images/train/'\n",
        "      mask_folder_path = root + 'Masks/train/'\n",
        "    else:\n",
        "      image_folder_path = root + 'Images/test/'\n",
        "      mask_folder_path = root + 'Masks/test/'\n",
        "\n",
        "    for image_name in os.listdir(image_folder_path):\n",
        "        if '.png' in image_name:\n",
        "          imgs.append(image_folder_path + image_name)\n",
        "\n",
        "          name = image_name[:-4]\n",
        "          masks.append(mask_folder_path + name + '_mask.png')\n",
        "\n",
        "    return imgs, masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttJMGzq0xniO"
      },
      "source": [
        "# отрисовка группы изображений\n",
        "\n",
        "def show_imgs(imgs):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(20,20))\n",
        "    for i, img in enumerate(imgs):\n",
        "        axs[0, i].imshow(img)\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfXrGrrVZu44"
      },
      "source": [
        "# получение бокса по маске\n",
        "# https://discuss.pytorch.org/t/extracting-bounding-box-coordinates-from-mask/61179/4\n",
        "\n",
        "def extract_boxes(mask):\n",
        "    mask = np.array(mask)\n",
        "    obj_ids = np.unique(mask)\n",
        "    obj_ids = obj_ids[1:]\n",
        "\n",
        "    masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "    num_objs = len(obj_ids)\n",
        "    boxes = []\n",
        "    for i in range(num_objs):\n",
        "        pos = np.where(masks[i])\n",
        "        xmin = np.min(pos[1])\n",
        "        xmax = np.max(pos[1])\n",
        "        ymin = np.min(pos[0])\n",
        "        ymax = np.max(pos[0])\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "    return boxes, num_objs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR7qbk5mBbld"
      },
      "source": [
        "transform_to_tensor = transforms.Compose([\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "transform_to_pil_img = transforms.Compose([\n",
        "    transforms.ToPILImage()])\n",
        "\n",
        "img_names, mask_names = get_imgs_and_masks('PennFudanPed/', False)\n",
        "drawn_masks = []\n",
        "drawn_boxes = []\n",
        "\n",
        "for i in range(5):\n",
        "  img = Image.open(img_names[i]).convert('RGB')\n",
        "  mask = Image.open(mask_names[i])\n",
        "  boxes, _ = extract_boxes(mask)\n",
        "  img = transform_to_tensor(img) * 255\n",
        "  img = torch.tensor(img, dtype=torch.uint8)\n",
        "\n",
        "  # https://pytorch.org/vision/stable/utils.html\n",
        "  drawn_boxes.append(torchvision.utils.draw_bounding_boxes(img, boxes, colors=[\"red\"]*len(boxes), width=2))\n",
        "\n",
        "  mask = transform_to_tensor(mask.convert('RGB'))\n",
        "  mask = torch.tensor(mask, dtype=torch.bool)\n",
        "\n",
        "  # https://pytorch.org/vision/stable/utils.html\n",
        "  drawn_masks.append(torchvision.utils.draw_segmentation_masks(img, mask))\n",
        "\n",
        "drawn_masks = [transform_to_pil_img(img) for img in drawn_masks]\n",
        "drawn_boxes = [transform_to_pil_img(img) for img in drawn_boxes]\n",
        "\n",
        "show_imgs(drawn_masks)\n",
        "show_imgs(drawn_boxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO_1olx2y-2U"
      },
      "source": [
        "### **Задание 2** (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF5U65Kd4dFC"
      },
      "source": [
        "Посмотрите, в каком виде данные нужны будут детектору и лоссу для работы. Напишите `Dataset`, заведите по нему loader-ы для трейна и теста, проверьте что всё ок."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLDCZTj8sIFy"
      },
      "source": [
        "class PedestrianSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms, is_train=True, class_label=1):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.is_train = is_train\n",
        "        self.class_label = class_label    # в данном случае у нас один класс 15 (person), удобно завести это поле, чтобы менять номер лейбла\n",
        "\n",
        "        imgs, masks = get_imgs_and_masks(self.root, self.is_train)\n",
        "        self.imgs = imgs\n",
        "        self.masks = masks\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img = Image.open(self.imgs[idx])\n",
        "        mask = Image.open(self.masks[idx])\n",
        "\n",
        "        boxes, num_objs = extract_boxes(mask)\n",
        "        original_dims = torch.FloatTensor([img.width, img.height, img.width, img.height]).unsqueeze(0)\n",
        "        boxes = boxes / original_dims\n",
        "\n",
        "        if self.transforms is not None:\n",
        "          img = self.transforms(img)\n",
        "\n",
        "        x = np.repeat(self.class_label, (num_objs,))\n",
        "        labels = torch.tensor(x, dtype=torch.int64)\n",
        "        \n",
        "        return img, boxes, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6-C3gUSPqUZ"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "                transforms.Resize((300, 300)),\n",
        "                transforms.ToTensor()])\n",
        "\n",
        "# для отрисовки\n",
        "transform_to_pil_img = transforms.Compose([\n",
        "    transforms.ToPILImage()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1Ludr94852g"
      },
      "source": [
        "trainset = PedestrianSet(root='PennFudanPed/', transforms=transform, is_train=True, class_label=15)\n",
        "testset = PedestrianSet(root='PennFudanPed/', transforms=transform, is_train=False, class_label=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8asfFvgYW8Y"
      },
      "source": [
        "def collate(batch):\n",
        "    imgs = []\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    for item in batch:\n",
        "        img, box, label = item\n",
        "        imgs.append(img)\n",
        "        boxes.append(box)\n",
        "        labels.append(label)\n",
        "\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "    return imgs, boxes, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cqSvJPD7fxt"
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, num_workers=8, shuffle=True, pin_memory=True, collate_fn=collate)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=10, num_workers=8, shuffle=False, pin_memory=True, collate_fn=collate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaYMKRY4XB6b"
      },
      "source": [
        "# посмотрим на то, что получилось в dataset\n",
        "\n",
        "pics = []\n",
        "for batch_imgs, batch_boxes, batch_labels in testloader:\n",
        "    for i, img in enumerate(batch_imgs):\n",
        "      name_labels = [rev_label_map[l.item()] for l in batch_labels[i]]\n",
        "      original_dims = torch.FloatTensor([300, 300, 300, 300]).unsqueeze(0)\n",
        "      batch_boxes[i] = batch_boxes[i] * original_dims\n",
        "      pic = draw_boxes(transform_to_pil_img(img), batch_boxes[i], name_labels, label_color_map)\n",
        "      pics.append(pic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1WrBhF3J2CJ"
      },
      "source": [
        "show_imgs(pics[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jAMBAlyOH2u"
      },
      "source": [
        "## **Часть 2. Копаемся в SSD**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdifHNcHly7I"
      },
      "source": [
        "Глобальная идея - мы хотим проверить следующую гипотезу: если взять предобученный детектор, оставить ему одну голову только на один класс и затюнить его на моноклассовый датасет, то на тесте этого датасета мы получим лучше результат, чем на нем давал этот детектор первоначально. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XNbMueImJbP"
      },
      "source": [
        "### **Задание 3**. (4 балла) \n",
        "\n",
        "Тогда сперва давайте получим результат детекции для класса `person` у первоначальной сети (никак не измененной) на `pedestrian test` сете. \n",
        "\n",
        "Что для этого нужно: \n",
        " - получить предсказания детектора на всех фотках теста (1 балл)\n",
        " - написать функцию для подсчета `AP` (2 балла)\n",
        " - посчитать `АР` для класса `person` при `IoU=0.75` с gt (1 балл)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb2FZCODm4xS"
      },
      "source": [
        "Ещё раз почитать про AP [тут](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173) или на русском то же самое [тут](https://ml.i-neti.com/2019/07/05/russkiy-map-mean-average-precision-v-detektsii-obektov/).\n",
        "\n",
        "Сразу скажу, что можно использовать код из [pycocotools](https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools), но не втупую. Возьмите оттуда только нужные для нашей задачи части. Во многих репозиториях вы увидите, что процедуры написаны специально для [coco](https://cocodataset.org/#home) - просто потому что это самый популярный датасет для детектирования и сегментации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTjia6eu9aIU"
      },
      "source": [
        "# предсказание на предобученной модели\n",
        "\n",
        "def get_prediction(testloader, model):\n",
        "  det_boxes_all = []\n",
        "  det_labels_all = []\n",
        "  det_scores_all = []\n",
        "  true_boxes_all = []\n",
        "  true_labels_all = []\n",
        "  imgs_all = []\n",
        "\n",
        "  for batch_imgs, batch_boxes, batch_labels in testloader:\n",
        "      batch_imgs = batch_imgs.to(device)\n",
        "      batch_boxes = [batch_boxes[k].to(device) for k in range(len(batch_boxes))]\n",
        "\n",
        "      locs, scores = model(batch_imgs)\n",
        "      det_boxes, det_labels, det_scores = model.detect_objects(locs.detach(), scores.detach(), min_score=0.5, max_overlap=0.5, top_k=10)\n",
        "      original_dims = torch.FloatTensor([300, 300, 300, 300]).unsqueeze(0).to(device)\n",
        "\n",
        "      det_labels_names = []\n",
        "      true_labels_names = []\n",
        "\n",
        "      for k in range(len(det_labels)):\n",
        "        det_labels_names.append([rev_label_map[l] for l in det_labels[k].to('cpu').tolist()])\n",
        "        true_labels_names.append([rev_label_map[l] for l in batch_labels[k].to('cpu').tolist()])\n",
        "\n",
        "      for k, _ in enumerate(det_boxes):\n",
        "        det_boxes[k] = det_boxes[k] * original_dims\n",
        "        batch_boxes[k] = batch_boxes[k] * original_dims\n",
        "        \n",
        "      det_boxes_all.append(det_boxes)\n",
        "      det_labels_all.append(det_labels_names)\n",
        "      det_scores_all.append(det_scores)\n",
        "      true_boxes_all.append(batch_boxes)\n",
        "      true_labels_all.append(true_labels_names)\n",
        "      imgs_all.append(batch_imgs)\n",
        "\n",
        "  return det_boxes_all, det_labels_all, det_scores_all, true_boxes_all, true_labels_all, imgs_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNyAZHYlWg6-"
      },
      "source": [
        "def calculate_AP(det_boxes, det_labels, det_scores, true_boxes, true_labels, class_label, iou_threshold, plot=False):\n",
        "\n",
        "  # отсортируем по скорам\n",
        "  scores = []\n",
        "  scores_dict = {}\n",
        "  for i in range(len(det_scores)):\n",
        "    if class_label in det_labels[i]:\n",
        "      scores.append(det_scores[i])\n",
        "      for j in range(det_scores[i].shape[0]):\n",
        "        scores_dict[i, j] = det_scores[i][j].item()\n",
        "\n",
        "  scores_list = torch.cat(scores).cpu()\n",
        "\n",
        "  sort_scores = np.sort(scores_list)[::-1]\n",
        "\n",
        "  scores_dict_reversed = dict(zip(scores_dict.values(), scores_dict.keys()))\n",
        "  \n",
        "  sort_inds = []\n",
        "  for el in sort_scores:\n",
        "    ind = scores_dict_reversed[el]\n",
        "    sort_inds.append(ind)\n",
        "\n",
        "  # общее кол-во gr\n",
        "  n = 0\n",
        "  match_boxes = []\n",
        "  for i in range(len(true_boxes)):\n",
        "    if class_label in true_labels[i]:\n",
        "      n += len(true_boxes[i])\n",
        "      match_boxes.append(np.zeros(len(true_labels[i])))\n",
        "\n",
        "  precision = []\n",
        "  recall = []\n",
        "  rec = 0\n",
        "  num = 1\n",
        "  correct = 0\n",
        "\n",
        "  img_det_counter = np.zeros(len(det_scores))\n",
        "\n",
        "  for ind in sort_inds:\n",
        "\n",
        "    img_ind, img_obj_ind = ind\n",
        "\n",
        "    img_det_counter[img_ind] += 1\n",
        "    \n",
        "    ious = []\n",
        "    for i in range(len(true_boxes[img_ind])):\n",
        "      iou = find_jaccard_overlap(true_boxes[img_ind][i][None, :].to(device), det_boxes[img_ind][img_obj_ind][None, :]).cpu()\n",
        "      ious.append(iou)\n",
        "\n",
        "    sort_inds = np.argsort(ious)[::-1]\n",
        "\n",
        "    fl = True\n",
        "    for i in sort_inds:\n",
        "\n",
        "      if fl:\n",
        "        if match_boxes[img_ind][i] == 0:\n",
        "          if ious[i] > iou_threshold:\n",
        "            correct += 1\n",
        "            match_boxes[img_ind][i] = 1\n",
        "            rec += 1 / n\n",
        "            fl = False\n",
        "\n",
        "          if not fl or img_det_counter[img_ind] == det_scores[img_ind].shape[0]:\n",
        "            # рассчитываем precision, recall\n",
        "            pr = correct / num\n",
        "            num += 1\n",
        "            precision.append(pr)\n",
        "            recall.append(rec)\n",
        "\n",
        "  # интерполируем полученный precision, чтобы получить строго монотонную функцию\n",
        "  for i in range(1, len(precision)-1)[::-1]:\n",
        "    if precision[i] != 1:\n",
        "      if precision[i+1] - precision[i] > 0:\n",
        "        precision[i] = precision[i+1]\n",
        "\n",
        "  # изобразим полученную кривую\n",
        "  if plot:\n",
        "    plt.plot(recall, precision)\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.show()\n",
        "\n",
        "  return np.trapz(precision, x=recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fox6cep06Kd3"
      },
      "source": [
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n32c5wajVA58"
      },
      "source": [
        "det_boxes_all, det_labels_all, det_scores_all, true_boxes_all, true_labels_all, imgs_all = get_prediction(testloader, ssd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLr1v1iN_21J"
      },
      "source": [
        "# посмотрим на точность при пороге 0.5\n",
        "\n",
        "ap = calculate_AP(flatten(det_boxes_all), flatten(det_labels_all), flatten(det_scores_all), \n",
        "                  flatten(true_boxes_all), flatten(true_labels_all), 'person', 0.5, True)\n",
        "\n",
        "print('Average precision for class \"Person\": ', ap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwS_YGiG_9eQ"
      },
      "source": [
        "# посмотрим на точность при пороге 0.75\n",
        "\n",
        "ap = calculate_AP(flatten(det_boxes_all), flatten(det_labels_all), flatten(det_scores_all), \n",
        "                  flatten(true_boxes_all), flatten(true_labels_all), 'person', 0.75, True)\n",
        "\n",
        "print('Average precision for class \"Person\": ', ap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij9E2RG_DxHS"
      },
      "source": [
        "pics = []\n",
        "k = 0\n",
        "for batch_imgs, batch_boxes, batch_labels in testloader:\n",
        "    for i in range(1):\n",
        "      name_labels = [rev_label_map[l.item()] for l in batch_labels[i]]\n",
        "      pic = draw_boxes(transform_to_pil_img(batch_imgs[i]), det_boxes_all[k][i], det_labels_all[k][i], label_color_map)\n",
        "      pics.append(pic)\n",
        "    k += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEtYiJAZkS_4"
      },
      "source": [
        "# посмотрим на предсказанные боксы\n",
        "\n",
        "show_imgs(pics[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdLGA9h_7ZHX"
      },
      "source": [
        "### **Задание 4**. (12 баллов) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVOKAgstpDHL"
      },
      "source": [
        "А вот теперь будем играться с моноклассовой модификацией детектора.\n",
        "\n",
        "1. Замените головы нашего `SSD300` так, чтобы теперь он детектировал только один класс. Вы можете создать новый экземпляр класса с другим параметром и аккуратно подтащить веса, либо уже у существующего `pretrained` детектора заменить необходимые слои (2 балла) \n",
        "\n",
        "2. Подготовьте код для обучения и тестирования. **Явно пропишите свои функции train_epoch** (3 балла), **evaluate** (2 балла).\n",
        "\n",
        "Процедуры обучения и эвала можно посмотреть тут: [тык 1](https://github.com/pytorch/vision/blob/master/references/detection/engine.py), [тык 2](https://github.com/multimodallearning/pytorch-mask-rcnn/blob/master/model.py#L1736), [тык 3](https://github.com/pytorch/vision/blob/0467c9d74c9b34f91df905ed8cf8433de48d7fa5/references/detection/coco_eval.py#L19). А ещё можно покопаться в одной из самых известных (но чуть более сложной чем у нас) реализаций [ssd на pytorch](https://github.com/amdegroot/ssd.pytorch). \n",
        "\n",
        "3. Поэкспериментируйте и постарайтесь выйти на хорошее качество на test - **bbox AP@IoU=0.75 >=0.95**. Попробуйте разные аугментации, lr, оптимайзеры, заморозку слоев и что ещё хочется (может быть самые смелые даже набор якорей попробуют поменять?). Напишите резюме - что получилсь, что нет, что зашло - а что не оч. (5 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BknR1kyNvk_i"
      },
      "source": [
        "Какой результат проверки гипотезы? Лучше ли тюнить детектор или и так было норм?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWK6ZVrvdq8J"
      },
      "source": [
        "ssd2 = SSD300(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wstBhySuOPos"
      },
      "source": [
        "# возьмем все веса, кроме pred_convs\n",
        "\n",
        "new_weights = {}\n",
        "for key in ssd2.state_dict():\n",
        "  if \"pred_convs\" not in key:\n",
        "    new_weights[key] = checkpoint['state_dict'][key]\n",
        "  else:\n",
        "    new_weights[key] = ssd2.state_dict()[key]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-SY8cHhOpdb"
      },
      "source": [
        "ssd2.load_state_dict(new_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTY4dC91xuRC"
      },
      "source": [
        "# заморозим веса\n",
        "\n",
        "# for name, param in ssd.named_parameters():\n",
        "#   if (\"pred_convs\" not in name) and (param.requires_grad):\n",
        "#     param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EduKiDIeF6B_"
      },
      "source": [
        "# переопределим label_map под новое число классов (background и person)\n",
        "\n",
        "label_map = {\"background\": 0, \"person\": 1}\n",
        "rev_label_map = {v: k for k, v in label_map.items()}\n",
        "\n",
        "# https://sashat.me/2017/01/11/list-of-20-simple-distinct-colors/\n",
        "distinct_colors = ['#e6194b', '#3cb44b']\n",
        "label_color_map = {k: distinct_colors[i] for i, k in enumerate(label_map.keys())}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3MCoS_Nr84A"
      },
      "source": [
        "trainset = PedestrianSet(root='PennFudanPed/', transforms=transform, is_train=True, class_label=1)\n",
        "testset = PedestrianSet(root='PennFudanPed/', transforms=transform, is_train=False, class_label=1)\n",
        "\n",
        "batch_size = 10\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, num_workers=8, shuffle=True, pin_memory=True, collate_fn=collate)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, num_workers=8, shuffle=False, pin_memory=True, collate_fn=collate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuyAc4O3f7Ll"
      },
      "source": [
        "n_epochs = 20\n",
        "\n",
        "optimizer = torch.optim.SGD(ssd2.parameters(), lr=1e-3, momentum=0.9)\n",
        "# optimizer = torch.optim.Adam(ssd2.parameters(), lr)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(len(trainset)/batch_size + 1)*n_epochs)\n",
        "criterion = MultiBoxLoss(ssd2.priors_cxcy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA7jVk4DiBZ3"
      },
      "source": [
        "class EmptyContext:\n",
        "    def __enter__(self):\n",
        "        pass\n",
        "    \n",
        "    def __exit__(self, *args):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvNdmrfrgZxA"
      },
      "source": [
        "# объединим train и eval в одну функцию (как в первой лекции), optimizer == None: eval\n",
        "\n",
        "def perform_epoch(net, dataloader, criterion, optimizer=None, device=None):\n",
        "\n",
        "    if device is not None:\n",
        "      net = net.to(device)\n",
        "\n",
        "    if optimizer is None:\n",
        "      net.eval()\n",
        "    else:\n",
        "      net.train()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    det_boxes_all = []\n",
        "    det_labels_all = []\n",
        "    det_scores_all = []\n",
        "    true_boxes_all = []\n",
        "    true_labels_all = []\n",
        "    imgs_all = []\n",
        "\n",
        "    with EmptyContext() if optimizer is not None else torch.no_grad():\n",
        "      for i, (batch_imgs, batch_boxes, batch_labels) in enumerate(dataloader):\n",
        "        \n",
        "        batch_imgs = batch_imgs.to(device)\n",
        "        batch_boxes = [b.to(device)  for b in batch_boxes]\n",
        "        batch_labels = [l.to(device) for l in batch_labels]\n",
        "\n",
        "        locs, scores = net(batch_imgs)\n",
        "        loss = criterion(locs, scores, batch_boxes, batch_labels)\n",
        "\n",
        "        # backprop\n",
        "        if optimizer is not None:\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        det_boxes, det_labels, det_scores = net.detect_objects(locs.detach(), scores.detach(), min_score=0.5, max_overlap=0.5, top_k=10)\n",
        "        original_dims = torch.FloatTensor([300, 300, 300, 300]).unsqueeze(0).to(device)\n",
        "\n",
        "        det_labels_names = []\n",
        "        for k in range(len(det_labels)):\n",
        "          det_labels_names.append([rev_label_map[l.item()] for l in det_labels[k]])\n",
        "\n",
        "        for k in range(len(det_boxes)):\n",
        "          det_boxes[k] = det_boxes[k] * original_dims\n",
        "          batch_boxes[k] = batch_boxes[k] * original_dims\n",
        "\n",
        "        true_labels_names = []\n",
        "        for k in range(len(batch_imgs)):\n",
        "          true_labels_names.append([rev_label_map[l.item()] for l in batch_labels[k]])\n",
        "\n",
        "        det_boxes_all.append(det_boxes)\n",
        "        det_labels_all.append(det_labels_names)\n",
        "        det_scores_all.append(det_scores)\n",
        "        true_boxes_all.append(batch_boxes)\n",
        "        true_labels_all.append(true_labels_names)\n",
        "        imgs_all.append(batch_imgs)\n",
        "\n",
        "    \n",
        "    ap = calculate_AP(flatten(det_boxes_all), flatten(det_labels_all), flatten(det_scores_all), \n",
        "                  flatten(true_boxes_all), flatten(true_labels_all), 'person', 0.75, False)\n",
        "    \n",
        "    return total_loss, ap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-vJ4dgqzSK4"
      },
      "source": [
        "ssd2 = ssd2.to(device)\n",
        "\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "train_aps = []\n",
        "test_aps = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    loss_train, ap_train = perform_epoch(ssd2, trainloader, criterion, optimizer=optimizer, device=device)\n",
        "    loss_test, ap_test = perform_epoch(ssd2, testloader, criterion, optimizer=None, device=device)\n",
        "    train_loss.append(loss_train)\n",
        "    test_loss.append(loss_test)\n",
        "    train_aps.append(ap_train)\n",
        "    test_aps.append(ap_test)\n",
        "\n",
        "    print(f\"Epoch: [{epoch+1} / {n_epochs}], Train loss: {round(loss_train,4)}, Train AP: {round(ap_train,4)}\")\n",
        "    print(f\"Epoch: [{epoch+1} / {n_epochs}], Test loss: {round(loss_test,4)}, Test AP: {round(ap_test,4)}\")\n",
        "    print('-------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4XNOmvXfhex"
      },
      "source": [
        "# сделаем предсказания на тесте\n",
        "det_boxes_all, det_labels_all, det_scores_all, true_boxes_all, true_labels_all, imgs_all = get_prediction(testloader, ssd2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15-Qtne6Ubq-"
      },
      "source": [
        "# ap, порог = 0.75\n",
        "\n",
        "ap = calculate_AP(flatten(det_boxes_all), flatten(det_labels_all), flatten(det_scores_all), \n",
        "                  flatten(true_boxes_all), flatten(true_labels_all), 'person', 0.75, True)\n",
        "print('Average precision for class \"Person\": ', ap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw03ARLoAsEr"
      },
      "source": [
        "# посмотрим на детекции\n",
        "\n",
        "pics = []\n",
        "k = 0\n",
        "for batch_imgs, batch_boxes, batch_labels in testloader:\n",
        "    for i in range(len(batch_imgs)):\n",
        "      name_labels = [rev_label_map[l.item()] for l in batch_labels[i]]\n",
        "      pic = draw_boxes(transform_to_pil_img(batch_imgs[i]), det_boxes_all[k][i], det_labels_all[k][i], label_color_map)\n",
        "      pics.append(pic)\n",
        "    k += 1\n",
        "\n",
        "show_imgs(pics[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWkkyA3ZCMAi"
      },
      "source": [
        "Кажется, что все не очень неплохо"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NrBgyh09wz1"
      },
      "source": [
        "**Резюме:**\n",
        "\n",
        "- Было ооочень непросто, но крайне интересно:)\n",
        "\n",
        "- Очень важно выбрать подходящий оптимизатор (Adam вообще не заходил)\n",
        "\n",
        "- Sheduler помогает\n",
        "\n",
        "- Заморозка весов не привела к значительному изменению метрик\n",
        "\n",
        "- Аугментацию уже не успела попробовать:("
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6WN99t5UPd0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}